{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfye+aUhZZ96Zht0lM7SiU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a8bffbf49c749ecb567ef5242988059": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4fd285db34874903b1219530ee490b1d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32mâ ¼\u001b[0m \u001b[1;32mThinking...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â ¼</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Thinking...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "4fd285db34874903b1219530ee490b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kethanvr/Reag-VecotrDB/blob/main/Final_Rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbgISdWqEo2R",
        "outputId": "1d896722-cfb8-4646-f56c-ccf4991b60a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dependencies installed successfully.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Required Packages\n",
        "!pip install -q pymongo sentence-transformers google-generativeai langchain langchain-google-genai langchain-community pypdf python-docx openpyxl pandas unstructured pillow langchain-text-splitters\n",
        "!pip install -q duckduckgo-search beautifulsoup4 requests\n",
        "!pip install -q rich\n",
        "\n",
        "print(\"âœ… Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Load API Keys & Imports\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import uuid\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict, Any, Optional\n",
        "from io import BytesIO\n",
        "\n",
        "# ML & AI Imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from pymongo import MongoClient\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, Docx2txtLoader, TextLoader,\n",
        "    UnstructuredExcelLoader, CSVLoader,\n",
        "    UnstructuredMarkdownLoader, UnstructuredHTMLLoader\n",
        ")\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Web & Search Imports\n",
        "from bs4 import BeautifulSoup\n",
        "from duckduckgo_search import DDGS\n",
        "from PIL import Image\n",
        "\n",
        "# UI Imports\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "\n",
        "# Configuration\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    MONGODB_URI = userdata.get('MONGODB_URI')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    print(\"âœ… API Keys loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading keys: {e}\")\n",
        "    print(\"Please set GEMINI_API_KEY and MONGODB_URI in Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKSVN2hbGRvO",
        "outputId": "edad8bf6-9131-42c6-9d21-89bebc6f9960"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API Keys loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Define Advanced RAG Class (Expanded File Support)\n",
        "class AdvancedRAGWithMemoryVisionWeb:\n",
        "    def __init__(self, mongodb_uri: str, db_name: str = \"rag\", collection_name: str = \"rag-collection\"):\n",
        "        \"\"\"Initialize Advanced RAG with Memory, Vision, and Web capabilities\"\"\"\n",
        "\n",
        "        # 1. Load Embedding Model\n",
        "        print(\"ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\")\n",
        "        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "        self.embedding_dim = 1024\n",
        "\n",
        "        # 2. Connect to MongoDB\n",
        "        print(\"ğŸ”„ Connecting to MongoDB Atlas...\")\n",
        "        self.client = MongoClient(mongodb_uri)\n",
        "        self.db = self.client[db_name]\n",
        "        self.collection = self.db[collection_name]\n",
        "        self.memory_collection = self.db[\"conversation_memory\"]\n",
        "        self.important_info_collection = self.db[\"important_info\"]\n",
        "\n",
        "        # 3. Initialize Gemini\n",
        "        self.model_name = 'gemini-2.5-flash-lite'\n",
        "        print(f\"ğŸ”„ Initializing {self.model_name}...\")\n",
        "        self.llm = genai.GenerativeModel(self.model_name)\n",
        "\n",
        "        # 4. Text Splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000, chunk_overlap=200, length_function=len\n",
        "        )\n",
        "\n",
        "        # 5. Session State\n",
        "        self.session_id = None\n",
        "        self.conversation_history = []\n",
        "\n",
        "        print(\"âœ… System Initialized!\")\n",
        "\n",
        "    # --- MEMORY ---\n",
        "    def start_new_session(self):\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "        self.conversation_history = []\n",
        "        return self.session_id\n",
        "\n",
        "    def add_to_memory(self, role: str, content: str):\n",
        "        msg = {\"role\": role, \"content\": content, \"timestamp\": str(uuid.uuid4())}\n",
        "        self.conversation_history.append(msg)\n",
        "        if self.session_id:\n",
        "            self.memory_collection.insert_one({**msg, \"session_id\": self.session_id})\n",
        "\n",
        "    def get_context_str(self, last_n=5):\n",
        "        return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.conversation_history[-last_n:]])\n",
        "\n",
        "    # --- VISION ---\n",
        "    def analyze_uploaded_image(self, uploaded_file_dict, question=\"Describe this image\"):\n",
        "        try:\n",
        "            filename = list(uploaded_file_dict.keys())[0]\n",
        "            image_data = uploaded_file_dict[filename]\n",
        "            image = Image.open(BytesIO(image_data))\n",
        "            response = self.llm.generate_content([question, image])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error processing image: {str(e)}\"\n",
        "\n",
        "    # --- WEB ---\n",
        "    def web_search_and_scrape(self, query: str, max_results=3):\n",
        "        try:\n",
        "            ddgs = DDGS()\n",
        "            results = list(ddgs.text(query, max_results=max_results))\n",
        "            if not results: return \"No web results found.\"\n",
        "\n",
        "            context = []\n",
        "            for r in results:\n",
        "                try:\n",
        "                    resp = requests.get(r['href'], timeout=5, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "                    soup = BeautifulSoup(resp.content, 'html.parser')\n",
        "                    text = soup.get_text(strip=True)[:1000]\n",
        "                    context.append(f\"Source: {r['title']}\\nURL: {r['href']}\\nContent: {text}\")\n",
        "                except:\n",
        "                    continue\n",
        "            return \"\\n\\n\".join(context)\n",
        "        except Exception as e:\n",
        "            return f\"Web search error: {str(e)}\"\n",
        "\n",
        "    # --- INGESTION (UPDATED FOR MORE FILE TYPES) ---\n",
        "    def ingest_file(self, file_path: str, metadata: Dict = None):\n",
        "        ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        # Define loaders for different types\n",
        "        if ext == '.pdf':\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif ext in ['.docx', '.doc']:\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        elif ext == '.csv':\n",
        "            loader = CSVLoader(file_path)\n",
        "        elif ext == '.md':\n",
        "            # Try UnstructuredMarkdown if available, else fallback to Text\n",
        "            try:\n",
        "                loader = UnstructuredMarkdownLoader(file_path)\n",
        "            except:\n",
        "                loader = TextLoader(file_path)\n",
        "        elif ext in ['.html', '.htm']:\n",
        "            loader = UnstructuredHTMLLoader(file_path)\n",
        "        elif ext in ['.txt', '.json', '.xml', '.py', '.js', '.java', '.c', '.cpp', '.yaml', '.yml', '.ini', '.log']:\n",
        "            # Generic Text Fallback for code and data files\n",
        "            loader = TextLoader(file_path)\n",
        "        else:\n",
        "            print(f\"âš ï¸ Skipping unsupported file: {os.path.basename(file_path)}\")\n",
        "            return 0\n",
        "\n",
        "        try:\n",
        "            docs = loader.load()\n",
        "            if not docs: return 0\n",
        "\n",
        "            chunks = self.text_splitter.split_documents(docs)\n",
        "\n",
        "            new_chunks = 0\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_hash = hashlib.sha256(chunk.page_content.encode()).hexdigest()\n",
        "\n",
        "                if self.collection.find_one({\"hash\": chunk_hash}):\n",
        "                    continue\n",
        "\n",
        "                doc = {\n",
        "                    \"text\": chunk.page_content,\n",
        "                    \"embedding\": self.embedding_model.encode(chunk.page_content).tolist(),\n",
        "                    \"hash\": chunk_hash,\n",
        "                    \"metadata\": {**(metadata or {}), \"chunk_index\": i}\n",
        "                }\n",
        "                self.collection.insert_one(doc)\n",
        "                new_chunks += 1\n",
        "            return new_chunks\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error ingesting {file_path}: {e}\")\n",
        "            return 0\n",
        "\n",
        "    # --- RETRIEVAL & GENERATION ---\n",
        "    def generate_answer(self, query: str, use_web=False):\n",
        "        self.add_to_memory(\"user\", query)\n",
        "\n",
        "        ctx_sources = []\n",
        "        web_content = \"\"\n",
        "\n",
        "        q_emb = self.embedding_model.encode(query).tolist()\n",
        "        try:\n",
        "            results = list(self.collection.aggregate([\n",
        "                {\"$vectorSearch\": {\n",
        "                    \"index\": \"vector_index\",\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"queryVector\": q_emb,\n",
        "                    \"numCandidates\": 50,\n",
        "                    \"limit\": 3\n",
        "                }},\n",
        "                {\"$project\": {\"_id\": 0, \"text\": 1, \"metadata\": 1, \"score\": {\"$meta\": \"vectorSearchScore\"}}}\n",
        "            ]))\n",
        "            ctx_sources = results\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Vector Search failed: {e}\")\n",
        "\n",
        "        if use_web or \"latest\" in query.lower() or \"news\" in query.lower():\n",
        "            web_content = self.web_search_and_scrape(query)\n",
        "\n",
        "        doc_text = \"\\n\\n\".join([f\"[Doc Source: {r['metadata'].get('source')}]\\n{r['text']}\" for r in ctx_sources])\n",
        "        history = self.get_context_str()\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are an advanced AI assistant. Use the following context to answer the user's question.\n",
        "\n",
        "        Conversation History:\n",
        "        {history}\n",
        "\n",
        "        Document Context:\n",
        "        {doc_text}\n",
        "\n",
        "        Web Context:\n",
        "        {web_content}\n",
        "\n",
        "        User Question: {query}\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.llm.generate_content(prompt)\n",
        "        answer = response.text\n",
        "        self.add_to_memory(\"assistant\", answer)\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": ctx_sources,\n",
        "            \"web_used\": bool(web_content)\n",
        "        }\n",
        "\n",
        "# Re-initialize\n",
        "rag = AdvancedRAGWithMemoryVisionWeb(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    db_name=\"rag\",\n",
        "    collection_name=\"rag-collection\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV4iqpIMGaaD",
        "outputId": "104d2ba5-8bf5-4284-e2cd-89f456dfc225"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\n",
            "ğŸ”„ Connecting to MongoDB Atlas...\n",
            "ğŸ”„ Initializing gemini-2.5-flash-lite...\n",
            "âœ… System Initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Initialize & Create Indexes\n",
        "# Initialize\n",
        "rag = AdvancedRAGWithMemoryVisionWeb(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    db_name=\"rag\",\n",
        "    collection_name=\"rag-collection\"\n",
        ")\n",
        "\n",
        "# Create Hash Index for Deduplication (Runs once)\n",
        "print(\"ğŸ”§ Ensuring hash index exists...\")\n",
        "try:\n",
        "    rag.collection.create_index(\"hash\", unique=True)\n",
        "    print(\"âœ… Hash index verified!\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "\n",
        "rag.start_new_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "drXHHOKWTGiQ",
        "outputId": "43dfb9bc-049e-4949-ded0-3809cdb87a5a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\n",
            "ğŸ”„ Connecting to MongoDB Atlas...\n",
            "ğŸ”„ Initializing gemini-2.5-flash-lite...\n",
            "âœ… System Initialized!\n",
            "ğŸ”§ Ensuring hash index exists...\n",
            "âœ… Hash index verified!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eabc0474-a96e-4e65-89a9-5b62902531a6'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ynUjAgn-tV1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Upload & Ingest Documents (FIXED)\n",
        "print(\"ğŸ“¤ Upload documents (PDF, DOCX, TXT, CSV)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "total_chunks = 0\n",
        "\n",
        "for filename, content in uploaded.items():\n",
        "    print(f\"Processing {filename}...\")\n",
        "\n",
        "    # Write to temp file\n",
        "    temp_path = f\"/tmp/{filename}\"\n",
        "    with open(temp_path, 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "    # Ingest\n",
        "    chunks_added = rag.ingest_file(temp_path, metadata={\"source\": filename})\n",
        "\n",
        "    # Safety Check: Ensure chunks_added is a number before adding\n",
        "    if chunks_added is not None:\n",
        "        total_chunks += chunks_added\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: No chunks returned for {filename}\")\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n",
        "\n",
        "print(f\"\\nâœ… Ingestion Complete! Added {total_chunks} new chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "himZqPYXmfuW",
        "outputId": "cddbae7f-7674-412a-cfb6-6dbc0026c672"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¤ Upload documents (PDF, DOCX, TXT, CSV)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b71f7e5d-2a19-4369-b88d-4b316c456360\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b71f7e5d-2a19-4369-b88d-4b316c456360\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tB8Y50o5sGg6lKsa Kethanvr.txt to tB8Y50o5sGg6lKsa Kethanvr (2).txt\n",
            "Processing tB8Y50o5sGg6lKsa Kethanvr (2).txt...\n",
            "\n",
            "âœ… Ingestion Complete! Added 1 new chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Interactive Chat Interface\n",
        "console = Console()\n",
        "\n",
        "console.print(Panel.fit(\n",
        "    \"[bold cyan]Advanced RAG System[/bold cyan]\\n\"\n",
        "    \"[dim]Commands:[/dim]\\n\"\n",
        "    \"â€¢ [green]Any question[/green]: Search docs + memory\\n\"\n",
        "    \"â€¢ [green]web: <query>[/green]: Force web search\\n\"\n",
        "    \"â€¢ [green]image[/green]: Upload and analyze image\\n\"\n",
        "    \"â€¢ [green]exit[/green]: Quit\",\n",
        "    title=\"ğŸš€ Ready\", border_style=\"cyan\"\n",
        "))\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        query = console.input(\"\\n[bold yellow]User ğŸ‘¤:[/bold yellow] \").strip()\n",
        "\n",
        "        if query.lower() in ['exit', 'quit', 'q']:\n",
        "            console.print(\"[bold red]ğŸ‘‹ Goodbye![/bold red]\")\n",
        "            break\n",
        "\n",
        "        if not query: continue\n",
        "\n",
        "        # --- IMAGE MODE ---\n",
        "        if query.lower() == 'image':\n",
        "            console.print(\"[yellow]ğŸ“¤ Upload an image...[/yellow]\")\n",
        "            img_upload = files.upload()\n",
        "            if img_upload:\n",
        "                q_img = console.input(\"[bold magenta]Question about image:[/bold magenta] \")\n",
        "                with console.status(\"[bold green]Analyzing Image...[/bold green]\"):\n",
        "                    ans = rag.analyze_uploaded_image(img_upload, q_img)\n",
        "                console.print(Panel(Markdown(ans), title=\"ğŸ–¼ï¸ Image Analysis\", border_style=\"magenta\"))\n",
        "            continue\n",
        "\n",
        "        # --- TEXT/WEB MODE ---\n",
        "        use_web = False\n",
        "        if query.startswith(\"web:\"):\n",
        "            use_web = True\n",
        "            query = query.replace(\"web:\", \"\").strip()\n",
        "\n",
        "        with console.status(\"[bold green]Thinking...[/bold green]\"):\n",
        "            result = rag.generate_answer(query, use_web=use_web)\n",
        "\n",
        "        # Display Answer\n",
        "        console.print(Panel(\n",
        "            Markdown(result['answer']),\n",
        "            title=\"ğŸ¤– AI Response\",\n",
        "            border_style=\"green\",\n",
        "            box=box.ROUNDED\n",
        "        ))\n",
        "\n",
        "        # Display Sources\n",
        "        if result['sources']:\n",
        "            table = Table(title=\"ğŸ“š Sources Used\", box=box.SIMPLE)\n",
        "            table.add_column(\"Score\", style=\"cyan\")\n",
        "            table.add_column(\"Source File\", style=\"magenta\")\n",
        "            table.add_column(\"Snippet\", style=\"dim\")\n",
        "\n",
        "            for s in result['sources']:\n",
        "                table.add_row(\n",
        "                    f\"{s.get('score', 0):.2f}\",\n",
        "                    s['metadata'].get('source', 'unknown'),\n",
        "                    s['text'][:60].replace(\"\\n\", \" \") + \"...\"\n",
        "                )\n",
        "            console.print(table)\n",
        "\n",
        "        if result['web_used']:\n",
        "            console.print(\"[dim]ğŸŒ Web content was used to answer this.[/dim]\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        break\n",
        "    except Exception as e:\n",
        "        console.print(f\"[red]Error: {e}[/red]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453,
          "referenced_widgets": [
            "6a8bffbf49c749ecb567ef5242988059",
            "4fd285db34874903b1219530ee490b1d"
          ]
        },
        "id": "YmxZkYeNPjTj",
        "outputId": "7cf03c4b-6370-448a-de8b-cf6dff0188ee"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m ğŸš€ Ready \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[1;36mAdvanced RAG System\u001b[0m                  \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[2mCommands:\u001b[0m                            \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mAny question\u001b[0m: Search docs + memory \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mweb: <query>\u001b[0m: Force web search     \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mimage\u001b[0m: Upload and analyze image    \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mexit\u001b[0m: Quit                         \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸš€ Ready â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Advanced RAG System</span>                  <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Commands:</span>                            <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">Any question</span>: Search docs + memory <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">web: &lt;query&gt;</span>: Force web search     <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">image</span>: Upload and analyze image    <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">exit</span>: Quit                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33mUser ğŸ‘¤:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">User ğŸ‘¤:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi who is kethan\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a8bffbf49c749ecb567ef5242988059"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[32mâ•­â”€\u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32m ğŸ¤– AI Response \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m Kethan is a Gen Z fullstack developer and AI enthusiast from India. He is currently a B.Tech student at CMRIT   \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m Bangalore, in his 6th semester, and is between 19-20 years old. Kethan is known for building scalable products, \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m particularly at the intersection of technology and health/pet tech. He is actively shipping real products, not  \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m just focusing on academic learning, with projects like CoCreateAI and MediScan under his belt. He is also       \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m active on platforms like GitHub, LinkedIn, YouTube, and Twitter, building a genuine personal brand.             \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¤– AI Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> Kethan is a Gen Z fullstack developer and AI enthusiast from India. He is currently a B.Tech student at CMRIT   <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> Bangalore, in his 6th semester, and is between 19-20 years old. Kethan is known for building scalable products, <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> particularly at the intersection of technology and health/pet tech. He is actively shipping real products, not  <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> just focusing on academic learning, with projects like CoCreateAI and MediScan under his belt. He is also       <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> active on platforms like GitHub, LinkedIn, YouTube, and Twitter, building a genuine personal brand.             <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                                ğŸ“š Sources Used                                                \u001b[0m\n",
              "                                                                                                               \n",
              " \u001b[1m \u001b[0m\u001b[1mScore\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSource File                      \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSnippet                                                        \u001b[0m\u001b[1m \u001b[0m \n",
              " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
              " \u001b[36m \u001b[0m\u001b[36m0.85 \u001b[0m\u001b[36m \u001b[0m \u001b[35m \u001b[0m\u001b[35m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[35m \u001b[0m \u001b[2m \u001b[0m\u001b[2mPart 2: Kethan's Profileâ€”Journey & Positioning  Background: ...\u001b[0m\u001b[2m \u001b[0m \n",
              " \u001b[36m \u001b[0m\u001b[36m0.83 \u001b[0m\u001b[36m \u001b[0m \u001b[35m \u001b[0m\u001b[35m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[35m \u001b[0m \u001b[2m \u001b[0m\u001b[2mKethan VR: A Comprehensive Research Profile of an Emerging T...\u001b[0m\u001b[2m \u001b[0m \n",
              " \u001b[36m \u001b[0m\u001b[36m0.82 \u001b[0m\u001b[36m \u001b[0m \u001b[35m \u001b[0m\u001b[35m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[35m \u001b[0m \u001b[2m \u001b[0m\u001b[2mOpportunity: Uber for pet services, AI-powered pet health, p...\u001b[0m\u001b[2m \u001b[0m \n",
              "                                                                                                               \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                ğŸ“š Sources Used                                                </span>\n",
              "                                                                                                               \n",
              " <span style=\"font-weight: bold\"> Score </span> <span style=\"font-weight: bold\"> Source File                       </span> <span style=\"font-weight: bold\"> Snippet                                                         </span> \n",
              " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
              " <span style=\"color: #008080; text-decoration-color: #008080\"> 0.85  </span> <span style=\"color: #800080; text-decoration-color: #800080\"> /tmp/Kethan VR A Comprehensive.md </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Part 2: Kethan's Profileâ€”Journey &amp; Positioning  Background: ... </span> \n",
              " <span style=\"color: #008080; text-decoration-color: #008080\"> 0.83  </span> <span style=\"color: #800080; text-decoration-color: #800080\"> /tmp/Kethan VR A Comprehensive.md </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Kethan VR: A Comprehensive Research Profile of an Emerging T... </span> \n",
              " <span style=\"color: #008080; text-decoration-color: #008080\"> 0.82  </span> <span style=\"color: #800080; text-decoration-color: #800080\"> /tmp/Kethan VR A Comprehensive.md </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Opportunity: Uber for pet services, AI-powered pet health, p... </span> \n",
              "                                                                                                               \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33mUser ğŸ‘¤:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">User ğŸ‘¤:</span> </pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}