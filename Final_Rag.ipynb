{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2jZ4PC3Yyi5eQ9FHjeh5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1830f074d3ee4efc8ecc2b4cf63b6035": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c6b8a06240124a4ab978f4c80270b5f6",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32mâ ¸\u001b[0m \u001b[1;32mThinking...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â ¸</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Thinking...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c6b8a06240124a4ab978f4c80270b5f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kethanvr/Reag-VecotrDB/blob/main/Final_Rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbgISdWqEo2R",
        "outputId": "d779c7f1-fb10-4eca-8862-f5f2e9712020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m719.1/719.1 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Dependencies installed successfully.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Required Packages\n",
        "!pip install -q pymongo sentence-transformers google-generativeai langchain langchain-google-genai langchain-community pypdf python-docx openpyxl pandas unstructured pillow langchain-text-splitters\n",
        "!pip install -q duckduckgo-search beautifulsoup4 requests\n",
        "!pip install -q rich\n",
        "\n",
        "print(\"âœ… Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEv7RdxWR9lW",
        "outputId": "19305881-b2be-41a3-88af-724b03ebff87"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.12/dist-packages (8.1.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.1)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Load API Keys & Imports\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import uuid\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict, Any, Optional\n",
        "from io import BytesIO\n",
        "\n",
        "# ML & AI Imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from pymongo import MongoClient\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, Docx2txtLoader, TextLoader,\n",
        "    UnstructuredExcelLoader, CSVLoader,\n",
        "    UnstructuredMarkdownLoader, UnstructuredHTMLLoader\n",
        ")\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Web & Search Imports\n",
        "from bs4 import BeautifulSoup\n",
        "from duckduckgo_search import DDGS\n",
        "from PIL import Image\n",
        "\n",
        "# UI Imports\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "\n",
        "# Configuration\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    MONGODB_URI = userdata.get('MONGODB_URI')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    print(\"âœ… API Keys loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading keys: {e}\")\n",
        "    print(\"Please set GEMINI_API_KEY and MONGODB_URI in Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKSVN2hbGRvO",
        "outputId": "e3f77bc4-a975-4a76-a9eb-b0b6407ac451"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API Keys loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Define Advanced RAG Class (Expanded File Support & Free Web Search)\n",
        "import os\n",
        "import uuid\n",
        "import hashlib\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from typing import List, Dict, Any\n",
        "from PIL import Image\n",
        "\n",
        "# GenAI & LangChain Imports\n",
        "import google.generativeai as genai\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymongo import MongoClient\n",
        "from duckduckgo_search import DDGS  # Required for free web search\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Document Loaders\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, Docx2txtLoader, CSVLoader, UnstructuredMarkdownLoader,\n",
        "    UnstructuredHTMLLoader, TextLoader\n",
        ")\n",
        "\n",
        "class AdvancedRAGWithMemoryVisionWeb:\n",
        "    def __init__(self, mongodb_uri: str, db_name: str = \"rag\", collection_name: str = \"rag-collection\"):\n",
        "        \"\"\"Initialize Advanced RAG with Memory, Vision, and Web capabilities\"\"\"\n",
        "\n",
        "        # 1. Load Embedding Model\n",
        "        print(\"ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\")\n",
        "        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "        self.embedding_dim = 1024\n",
        "\n",
        "        # 2. Connect to MongoDB\n",
        "        print(\"ğŸ”„ Connecting to MongoDB Atlas...\")\n",
        "        self.client = MongoClient(mongodb_uri)\n",
        "        self.db = self.client[db_name]\n",
        "        self.collection = self.db[collection_name]\n",
        "        self.memory_collection = self.db[\"conversation_memory\"]\n",
        "\n",
        "        # 3. Initialize Gemini\n",
        "        self.model_name = 'gemini-2.5-flash-lite'\n",
        "        print(f\"ğŸ”„ Initializing {self.model_name}...\")\n",
        "        self.llm = genai.GenerativeModel(self.model_name)\n",
        "\n",
        "        # 4. Text Splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000, chunk_overlap=200, length_function=len\n",
        "        )\n",
        "\n",
        "        # 5. Session State\n",
        "        self.session_id = None\n",
        "        self.conversation_history = []\n",
        "\n",
        "        print(\"âœ… System Initialized!\")\n",
        "\n",
        "    # --- MEMORY ---\n",
        "    def start_new_session(self):\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "        self.conversation_history = []\n",
        "        return self.session_id\n",
        "\n",
        "    def add_to_memory(self, role: str, content: str):\n",
        "        msg = {\"role\": role, \"content\": content, \"timestamp\": str(uuid.uuid4())}\n",
        "        self.conversation_history.append(msg)\n",
        "        if self.session_id:\n",
        "            self.memory_collection.insert_one({**msg, \"session_id\": self.session_id})\n",
        "\n",
        "    def get_context_str(self, last_n=5):\n",
        "        return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.conversation_history[-last_n:]])\n",
        "\n",
        "    # --- VISION ---\n",
        "    def analyze_uploaded_image(self, uploaded_file_dict, question=\"Describe this image\"):\n",
        "        try:\n",
        "            filename = list(uploaded_file_dict.keys())[0]\n",
        "            image_data = uploaded_file_dict[filename]\n",
        "            image = Image.open(BytesIO(image_data))\n",
        "            response = self.llm.generate_content([question, image])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error processing image: {str(e)}\"\n",
        "\n",
        "    # --- WEB (UPDATED FOR DUCKDUCKGO) ---\n",
        "    def web_search_and_scrape(self, query: str, max_results=5):\n",
        "        \"\"\"\n",
        "        Performs a free web search using DuckDuckGo and returns summaries.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(f\"ğŸŒ Searching DuckDuckGo for: '{query}'...\")\n",
        "            context = []\n",
        "\n",
        "            # Use the context manager to ensure the session is handled correctly\n",
        "            with DDGS() as ddgs:\n",
        "                # DuckDuckGo returns keys: 'title', 'href', 'body' (snippet)\n",
        "                results = list(ddgs.text(query, max_results=max_results))\n",
        "\n",
        "                if not results:\n",
        "                    return \"No web results found.\"\n",
        "\n",
        "                for r in results:\n",
        "                    # We use the 'body' provided by DDG directly.\n",
        "                    # This is faster and avoids 403 Forbidden errors from scraping raw HTML.\n",
        "                    source_info = (\n",
        "                        f\"Source: {r.get('title', 'Unknown')}\\n\"\n",
        "                        f\"URL: {r.get('href', '#')}\\n\"\n",
        "                        f\"Content Summary: {r.get('body', '')}\"\n",
        "                    )\n",
        "                    context.append(source_info)\n",
        "\n",
        "            return \"\\n\\n\".join(context)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Web search error: {e}\")\n",
        "            return f\"Web search failed: {str(e)}\"\n",
        "\n",
        "    # --- INGESTION ---\n",
        "    def ingest_file(self, file_path: str, metadata: Dict = None):\n",
        "        ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        # Define loaders for different types\n",
        "        if ext == '.pdf':\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif ext in ['.docx', '.doc']:\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        elif ext == '.csv':\n",
        "            loader = CSVLoader(file_path)\n",
        "        elif ext == '.md':\n",
        "            try:\n",
        "                loader = UnstructuredMarkdownLoader(file_path)\n",
        "            except:\n",
        "                loader = TextLoader(file_path)\n",
        "        elif ext in ['.html', '.htm']:\n",
        "            loader = UnstructuredHTMLLoader(file_path)\n",
        "        elif ext in ['.txt', '.json', '.xml', '.py', '.js', '.java', '.c', '.cpp', '.yaml', '.yml', '.ini', '.log']:\n",
        "            loader = TextLoader(file_path)\n",
        "        else:\n",
        "            print(f\"âš ï¸ Skipping unsupported file: {os.path.basename(file_path)}\")\n",
        "            return 0\n",
        "\n",
        "        try:\n",
        "            docs = loader.load()\n",
        "            if not docs: return 0\n",
        "\n",
        "            chunks = self.text_splitter.split_documents(docs)\n",
        "\n",
        "            new_chunks = 0\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_hash = hashlib.sha256(chunk.page_content.encode()).hexdigest()\n",
        "\n",
        "                if self.collection.find_one({\"hash\": chunk_hash}):\n",
        "                    continue\n",
        "\n",
        "                doc = {\n",
        "                    \"text\": chunk.page_content,\n",
        "                    \"embedding\": self.embedding_model.encode(chunk.page_content).tolist(),\n",
        "                    \"hash\": chunk_hash,\n",
        "                    \"metadata\": {**(metadata or {}), \"chunk_index\": i}\n",
        "                }\n",
        "                self.collection.insert_one(doc)\n",
        "                new_chunks += 1\n",
        "            return new_chunks\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error ingesting {file_path}: {e}\")\n",
        "            return 0\n",
        "\n",
        "    # --- RETRIEVAL & GENERATION ---\n",
        "    def generate_answer(self, query: str, use_web=False):\n",
        "        self.add_to_memory(\"user\", query)\n",
        "\n",
        "        ctx_sources = []\n",
        "        web_content = \"\"\n",
        "\n",
        "        # 1. Vector Search (Internal Docs)\n",
        "        q_emb = self.embedding_model.encode(query).tolist()\n",
        "        try:\n",
        "            results = list(self.collection.aggregate([\n",
        "                {\"$vectorSearch\": {\n",
        "                    \"index\": \"vector_index\",\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"queryVector\": q_emb,\n",
        "                    \"numCandidates\": 50,\n",
        "                    \"limit\": 3\n",
        "                }},\n",
        "                {\"$project\": {\"_id\": 0, \"text\": 1, \"metadata\": 1, \"score\": {\"$meta\": \"vectorSearchScore\"}}}\n",
        "            ]))\n",
        "            ctx_sources = results\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Vector Search failed: {e}\")\n",
        "\n",
        "        # 2. Web Search Logic\n",
        "        # Trigger if explicitly requested OR keywords present OR internal docs are empty\n",
        "        triggers = [\"latest\", \"news\", \"current\", \"today\", \"who is\", \"what is\"]\n",
        "        if use_web or any(t in query.lower() for t in triggers):\n",
        "            print(\"ğŸ” Triggering Web Search...\")\n",
        "            web_content = self.web_search_and_scrape(query)\n",
        "\n",
        "        doc_text = \"\\n\\n\".join([f\"[Doc Source: {r['metadata'].get('source')}]\\n{r['text']}\" for r in ctx_sources])\n",
        "        history = self.get_context_str()\n",
        "\n",
        "        # 3. Construct Prompt\n",
        "        prompt = f\"\"\"\n",
        "        You are an advanced AI assistant. Answer the user's question using the context provided.\n",
        "\n",
        "        If the internal documents answer the question, prioritize them.\n",
        "        If the web context is needed for recent events or general knowledge, use it.\n",
        "\n",
        "        Conversation History:\n",
        "        {history}\n",
        "\n",
        "        Internal Document Context:\n",
        "        {doc_text if doc_text else \"No relevant internal documents found.\"}\n",
        "\n",
        "        Web Search Context:\n",
        "        {web_content if web_content else \"No web search performed.\"}\n",
        "\n",
        "        User Question: {query}\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.llm.generate_content(prompt)\n",
        "        answer = response.text\n",
        "        self.add_to_memory(\"assistant\", answer)\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": ctx_sources,\n",
        "            \"web_used\": bool(web_content)\n",
        "        }\n",
        "\n",
        "# Re-initialize\n",
        "rag = AdvancedRAGWithMemoryVisionWeb(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    db_name=\"rag\",\n",
        "    collection_name=\"rag-collection\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV4iqpIMGaaD",
        "outputId": "26ccbb9a-c4ac-42ed-cf95-c209a5310511"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\n",
            "ğŸ”„ Connecting to MongoDB Atlas...\n",
            "ğŸ”„ Initializing gemini-2.5-flash-lite...\n",
            "âœ… System Initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Initialize & Create Indexes\n",
        "# Initialize\n",
        "rag = AdvancedRAGWithMemoryVisionWeb(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    db_name=\"rag\",\n",
        "    collection_name=\"rag-collection\"\n",
        ")\n",
        "\n",
        "# Create Hash Index for Deduplication (Runs once)\n",
        "print(\"ğŸ”§ Ensuring hash index exists...\")\n",
        "try:\n",
        "    rag.collection.create_index(\"hash\", unique=True)\n",
        "    print(\"âœ… Hash index verified!\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "\n",
        "rag.start_new_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "drXHHOKWTGiQ",
        "outputId": "c243f26c-3f61-4488-84db-e9a2c75e6fba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\n",
            "ğŸ”„ Connecting to MongoDB Atlas...\n",
            "ğŸ”„ Initializing gemini-2.5-flash-lite...\n",
            "âœ… System Initialized!\n",
            "ğŸ”§ Ensuring hash index exists...\n",
            "âœ… Hash index verified!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'afb2bd9d-987c-42bd-9f6d-9dd30534339b'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ynUjAgn-tV1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Upload & Ingest Documents (FIXED)\n",
        "print(\"ğŸ“¤ Upload documents (PDF, DOCX, TXT, CSV)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "total_chunks = 0\n",
        "\n",
        "for filename, content in uploaded.items():\n",
        "    print(f\"Processing {filename}...\")\n",
        "\n",
        "    # Write to temp file\n",
        "    temp_path = f\"/tmp/{filename}\"\n",
        "    with open(temp_path, 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "    # Ingest\n",
        "    chunks_added = rag.ingest_file(temp_path, metadata={\"source\": filename})\n",
        "\n",
        "    # Safety Check: Ensure chunks_added is a number before adding\n",
        "    if chunks_added is not None:\n",
        "        total_chunks += chunks_added\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: No chunks returned for {filename}\")\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n",
        "\n",
        "print(f\"\\nâœ… Ingestion Complete! Added {total_chunks} new chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "himZqPYXmfuW",
        "outputId": "c8a3e6c7-90ea-4b3d-faec-1f56cb3910f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¤ Upload documents (PDF, DOCX, TXT, CSV)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ffb4dc59-26c0-4f96-9d46-b70fb6c362bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ffb4dc59-26c0-4f96-9d46-b70fb6c362bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Kethan VR A Comprehensive.md to Kethan VR A Comprehensive.md\n",
            "Processing Kethan VR A Comprehensive.md...\n",
            "\n",
            "âœ… Ingestion Complete! Added 0 new chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Interactive Chat Interface (Zero-Noise Mode)\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# --- NUCLEAR OPTION: SUPPRESS ALL NOISE ---\n",
        "# 1. Standard Python Warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "\n",
        "# 2. Silence Third-Party Loggers\n",
        "logging.getLogger(\"tornado.access\").setLevel(logging.CRITICAL)\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.CRITICAL)\n",
        "logging.getLogger(\"duckduckgo_search\").setLevel(logging.CRITICAL)\n",
        "\n",
        "# 3. Context Manager to eat stderr during noisy operations\n",
        "@contextmanager\n",
        "def suppress_stderr():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "from google.colab import files\n",
        "\n",
        "console = Console()\n",
        "\n",
        "console.print(Panel.fit(\n",
        "    \"[bold cyan]Advanced RAG System[/bold cyan]\\n\"\n",
        "    \"[dim]Commands:[/dim]\\n\"\n",
        "    \"â€¢ [green]Any question[/green]: Search docs + memory\\n\"\n",
        "    \"â€¢ [green]web: <query>[/green]: Force web search\\n\"\n",
        "    \"â€¢ [green]image[/green]: Upload and analyze image\\n\"\n",
        "    \"â€¢ [green]exit[/green]: Quit\",\n",
        "    title=\"ğŸš€ Ready (Zero-Noise Mode)\", border_style=\"cyan\"\n",
        "))\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # Use standard input() to avoid Colab recursion errors\n",
        "        print(\"\\n\" + \"â”€\"*50)\n",
        "        query = input(\"User ğŸ‘¤ (Type 'exit' to quit): \").strip()\n",
        "\n",
        "        if query.lower() in ['exit', 'quit', 'q']:\n",
        "            console.print(\"[bold red]ğŸ‘‹ Goodbye![/bold red]\")\n",
        "            break\n",
        "\n",
        "        if not query: continue\n",
        "\n",
        "        # --- IMAGE MODE ---\n",
        "        if query.lower() == 'image':\n",
        "            console.print(\"[yellow]ğŸ“¤ Upload an image...[/yellow]\")\n",
        "            img_upload = files.upload()\n",
        "            if img_upload:\n",
        "                q_img = input(\"Question about image: \")\n",
        "                with console.status(\"[bold green]Analyzing Image...[/bold green]\"):\n",
        "                    with suppress_stderr(): # SILENCE WARNINGS HERE\n",
        "                        ans = rag.analyze_uploaded_image(img_upload, q_img)\n",
        "                console.print(Panel(Markdown(ans), title=\"ğŸ–¼ï¸ Image Analysis\", border_style=\"magenta\"))\n",
        "            continue\n",
        "\n",
        "        # --- TEXT/WEB MODE ---\n",
        "        use_web = False\n",
        "        if query.startswith(\"web:\"):\n",
        "            use_web = True\n",
        "            query = query.replace(\"web:\", \"\").strip()\n",
        "\n",
        "        with console.status(\"[bold green]Thinking...[/bold green]\"):\n",
        "            try:\n",
        "                # WRAP GENERATION IN SILENCE\n",
        "                # This eats the \"duckduckgo\" and \"ipywidgets\" red text\n",
        "                with suppress_stderr():\n",
        "                    if hasattr(rag, 'web_search_and_scrape'):\n",
        "                         result = rag.generate_answer(query)\n",
        "                    else:\n",
        "                         result = rag.generate_answer(query)\n",
        "\n",
        "                # Display Answer\n",
        "                console.print(Panel(\n",
        "                    Markdown(result['answer']),\n",
        "                    title=\"ğŸ¤– AI Response\",\n",
        "                    border_style=\"green\",\n",
        "                    box=box.ROUNDED\n",
        "                ))\n",
        "\n",
        "                # Display Sources\n",
        "                if result.get('sources'):\n",
        "                    table = Table(title=\"ğŸ“š Sources Used\", box=box.SIMPLE)\n",
        "                    table.add_column(\"Score\", style=\"cyan\")\n",
        "                    table.add_column(\"Source File\", style=\"magenta\")\n",
        "\n",
        "                    for s in result['sources']:\n",
        "                        table.add_row(\n",
        "                            f\"{s.get('score', 0):.2f}\",\n",
        "                            s['metadata'].get('source', 'unknown')\n",
        "                        )\n",
        "                    console.print(table)\n",
        "\n",
        "                if result.get('web_used'):\n",
        "                    console.print(\"[dim]ğŸŒ Web content was used to answer this.[/dim]\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # We turn stderr back on for actual errors so you can see them\n",
        "                console.print(f\"[red]Generation Error: {e}[/red]\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        console.print(\"\\n[bold red]ğŸ‘‹ Goodbye![/bold red]\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        console.print(f\"[red]System Error: {e}[/red]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562,
          "referenced_widgets": [
            "1830f074d3ee4efc8ecc2b4cf63b6035",
            "c6b8a06240124a4ab978f4c80270b5f6"
          ]
        },
        "id": "hml6bMSzQVhX",
        "outputId": "f8479c67-55d1-4a19-962f-470a0379b058"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€\u001b[0m\u001b[36m ğŸš€ Ready (Zero-Noise Mode) \u001b[0m\u001b[36mâ”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[1;36mAdvanced RAG System\u001b[0m                  \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[2mCommands:\u001b[0m                            \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mAny question\u001b[0m: Search docs + memory \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mweb: <query>\u001b[0m: Force web search     \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mimage\u001b[0m: Upload and analyze image    \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mexit\u001b[0m: Quit                         \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€ ğŸš€ Ready (Zero-Noise Mode) â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Advanced RAG System</span>                  <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Commands:</span>                            <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">Any question</span>: Search docs + memory <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">web: &lt;query&gt;</span>: Force web search     <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">image</span>: Upload and analyze image    <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">exit</span>: Quit                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "User ğŸ‘¤ (Type 'exit' to quit): who is modi\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1830f074d3ee4efc8ecc2b4cf63b6035"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸ” Triggering Web Search...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ” Triggering Web Search...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸŒ Searching DuckDuckGo for: 'who is modi'...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸŒ Searching DuckDuckGo for: 'who is modi'...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[32mâ•­â”€\u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32m ğŸ¤– AI Response \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m I apologize, but the provided context does not contain any information about \"Modi.\" The documents discuss      \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m Kethan VR, the Indian startup ecosystem, Gen Z founders, and the AI-first SaaS opportunity in India.            \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¤– AI Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> I apologize, but the provided context does not contain any information about \"Modi.\" The documents discuss      <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> Kethan VR, the Indian startup ecosystem, Gen Z founders, and the AI-first SaaS opportunity in India.            <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m               ğŸ“š Sources Used               \u001b[0m\n",
              "                                             \n",
              " \u001b[1m \u001b[0m\u001b[1mScore\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSource File                      \u001b[0m\u001b[1m \u001b[0m \n",
              " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
              " \u001b[36m \u001b[0m\u001b[36m0.75 \u001b[0m\u001b[36m \u001b[0m \u001b[35m \u001b[0m\u001b[35m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[35m \u001b[0m \n",
              " \u001b[36m \u001b[0m\u001b[36m0.74 \u001b[0m\u001b[36m \u001b[0m \u001b[35m \u001b[0m\u001b[35m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[35m \u001b[0m \n",
              " \u001b[36m \u001b[0m\u001b[36m0.74 \u001b[0m\u001b[36m \u001b[0m \u001b[35m \u001b[0m\u001b[35m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[35m \u001b[0m \n",
              "                                             \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">               ğŸ“š Sources Used               </span>\n",
              "                                             \n",
              " <span style=\"font-weight: bold\"> Score </span> <span style=\"font-weight: bold\"> Source File                       </span> \n",
              " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
              " <span style=\"color: #008080; text-decoration-color: #008080\"> 0.75  </span> <span style=\"color: #800080; text-decoration-color: #800080\"> /tmp/Kethan VR A Comprehensive.md </span> \n",
              " <span style=\"color: #008080; text-decoration-color: #008080\"> 0.74  </span> <span style=\"color: #800080; text-decoration-color: #800080\"> /tmp/Kethan VR A Comprehensive.md </span> \n",
              " <span style=\"color: #008080; text-decoration-color: #008080\"> 0.74  </span> <span style=\"color: #800080; text-decoration-color: #800080\"> /tmp/Kethan VR A Comprehensive.md </span> \n",
              "                                             \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mğŸŒ Web content was used to answer this.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">ğŸŒ Web content was used to answer this.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7970fe270520>\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;31mğŸ‘‹ Goodbye!\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ğŸ‘‹ Goodbye!</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}