{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu6QBNKTvd0ukkZUGepvgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kethanvr/Reag-VecotrDB/blob/main/Final_Rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbgISdWqEo2R",
        "outputId": "828e7be9-84e8-4fe2-c15b-ae59a410cb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.3/1.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/331.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pymongo sentence-transformers google-generativeai python-dotenv numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from pymongo import MongoClient\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKSVN2hbGRvO",
        "outputId": "02717c59-b7ae-4ed7-9b08-42c4d8a81b5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')  # Your Gemini API key\n",
        "MONGODB_URI = userdata.get('MONGODB_URI')  # Your MongoDB Atlas connection string\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "MV4iqpIMGaaD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    def __init__(self, mongodb_uri: str, db_name: str = \"rag_db\", collection_name: str = \"documents\"):\n",
        "        \"\"\"Initialize RAG system with BGE embeddings and MongoDB\"\"\"\n",
        "\n",
        "        # 1. Load BGE embedding model (1024 dimensions)\n",
        "        print(\"Loading BGE-large model (1024D)...\")\n",
        "        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "        self.embedding_dim = 1024\n",
        "\n",
        "        # 2. Initialize MongoDB connection\n",
        "        print(\"Connecting to MongoDB Atlas...\")\n",
        "        self.client = MongoClient(MONGODB_URI)\n",
        "        self.db = self.client[db_name]\n",
        "        self.collection = self.db[collection_name]\n",
        "\n",
        "        # 3. Initialize Gemini model\n",
        "        print(\"Initializing Gemini model...\")\n",
        "        self.llm = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "\n",
        "        print(\"âœ… RAG System initialized successfully!\")\n",
        "\n",
        "    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(words), chunk_size - overlap):\n",
        "            chunk = ' '.join(words[i:i + chunk_size])\n",
        "            if chunk.strip():\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def generate_hash(self, text: str) -> str:\n",
        "        \"\"\"Generate SHA-256 hash for deduplication\"\"\"\n",
        "        return hashlib.sha256(text.encode()).hexdigest()\n",
        "\n",
        "    def ingest_document(self, text: str, metadata: Dict[str, Any] = None):\n",
        "        \"\"\"Ingest a document: chunk, embed, deduplicate, and store\"\"\"\n",
        "        if metadata is None:\n",
        "            metadata = {}\n",
        "\n",
        "        # Step 1: Chunk the document\n",
        "        print(f\"Chunking document...\")\n",
        "        chunks = self.chunk_text(text)\n",
        "        print(f\"Created {len(chunks)} chunks\")\n",
        "\n",
        "        # Step 2 & 3: Generate embeddings and hashes\n",
        "        print(\"Generating embeddings and hashes...\")\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            # Generate embedding (1024D vector)\n",
        "            embedding = self.embedding_model.encode(chunk).tolist()\n",
        "\n",
        "            # Generate dedup hash\n",
        "            chunk_hash = self.generate_hash(chunk)\n",
        "\n",
        "            # Check if chunk already exists (deduplication)\n",
        "            existing = self.collection.find_one({\"hash\": chunk_hash})\n",
        "            if existing:\n",
        "                print(f\"Chunk {i+1} already exists (deduplicated)\")\n",
        "                continue\n",
        "\n",
        "            # Store in MongoDB\n",
        "            document = {\n",
        "                \"text\": chunk,\n",
        "                \"embedding\": embedding,\n",
        "                \"hash\": chunk_hash,\n",
        "                \"metadata\": {**metadata, \"chunk_index\": i}\n",
        "            }\n",
        "\n",
        "            self.collection.insert_one(document)\n",
        "            print(f\"âœ“ Stored chunk {i+1}/{len(chunks)}\")\n",
        "\n",
        "        print(\"âœ… Document ingestion complete!\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Perform semantic search using vector similarity\"\"\"\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_model.encode(query).tolist()\n",
        "\n",
        "        # MongoDB Vector Search Pipeline\n",
        "        pipeline = [\n",
        "            {\n",
        "                \"$vectorSearch\": {\n",
        "                    \"index\": \"vector_index\",\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"queryVector\": query_embedding,\n",
        "                    \"numCandidates\": top_k * 10,\n",
        "                    \"limit\": top_k\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"$project\": {\n",
        "                    \"_id\": 0,\n",
        "                    \"text\": 1,\n",
        "                    \"metadata\": 1,\n",
        "                    \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            results = list(self.collection.aggregate(pipeline))\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Search error: {e}\")\n",
        "            print(\"Note: Make sure vector search index 'vector_index' is created in MongoDB Atlas!\")\n",
        "            return []\n",
        "\n",
        "    def generate_answer(self, query: str, top_k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Generate answer using RAG approach\"\"\"\n",
        "        # Step 1: Retrieve relevant context\n",
        "        print(f\"ðŸ” Searching for relevant context...\")\n",
        "        search_results = self.search(query, top_k=top_k)\n",
        "\n",
        "        if not search_results:\n",
        "            return {\n",
        "                \"answer\": \"I couldn't find relevant information to answer your question. Please make sure the vector search index is set up in MongoDB Atlas.\",\n",
        "                \"sources\": [],\n",
        "                \"context_used\": [],\n",
        "                \"num_sources\": 0  # ADD THIS LINE\n",
        "            }\n",
        "\n",
        "        # Step 2: Prepare context\n",
        "        context_chunks = [result['text'] for result in search_results]\n",
        "        context = \"\\n\\n\".join([f\"[{i+1}] {chunk}\" for i, chunk in enumerate(context_chunks)])\n",
        "\n",
        "        # Step 3: Create prompt\n",
        "        prompt = f\"\"\"You are a helpful assistant. Answer the question based ONLY on the provided context.\n",
        "If the context doesn't contain enough information, say so. Do not make up information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Step 4: Generate answer using Gemini\n",
        "        print(\"ðŸ¤– Generating answer with Gemini...\")\n",
        "        response = self.llm.generate_content(prompt)\n",
        "        answer = response.text\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": search_results,\n",
        "            \"context_used\": context_chunks,\n",
        "            \"num_sources\": len(search_results)\n",
        "        }"
      ],
      "metadata": {
        "id": "RWvNksumG-Dn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BKMtPwH5KU6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Usage Example\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INITIALIZING RAG SYSTEM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize RAG system\n",
        "rag = RAGSystem(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    db_name=\"rag\",\n",
        "    collection_name=\"rag-collection\"\n",
        ")\n",
        "\n",
        "# Example document\n",
        "sample_document = \"\"\"\n",
        "Artificial Intelligence (AI) is transforming the world. Machine learning, a subset of AI,\n",
        "enables computers to learn from data without explicit programming. Deep learning uses neural\n",
        "networks with multiple layers to process complex patterns. Natural Language Processing (NLP)\n",
        "helps machines understand human language. Computer vision allows machines to interpret images\n",
        "and videos. AI applications include healthcare diagnosis, autonomous vehicles, recommendation\n",
        "systems, and virtual assistants. The field of AI continues to evolve rapidly with new\n",
        "breakthroughs in areas like reinforcement learning, generative AI, and transfer learning.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INGESTING DOCUMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rag.ingest_document(\n",
        "    text=sample_document,\n",
        "    metadata={\"source\": \"AI Overview\", \"topic\": \"Artificial Intelligence\"}\n",
        ")\n",
        "\n",
        "# Ask a question\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ASKING QUESTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "question = \"What is machine learning?\"\n",
        "result = rag.generate_answer(question, top_k=3)\n",
        "\n",
        "print(f\"\\nðŸ“ Question: {question}\")\n",
        "\n",
        "# Safe access to result dictionary\n",
        "if result.get('answer'):\n",
        "    print(f\"\\nâœ… Answer:\\n{result['answer']}\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ No answer generated\")\n",
        "\n",
        "# Safely check for sources\n",
        "num_sources = len(result.get('sources', []))\n",
        "print(f\"\\nðŸ“š Used {num_sources} source(s)\")\n",
        "\n",
        "# Safely display context\n",
        "context_used = result.get('context_used', [])\n",
        "if context_used:\n",
        "    print(\"\\nðŸ” Retrieved Context:\")\n",
        "    for i, context in enumerate(context_used, 1):\n",
        "        print(f\"\\n[{i}] {context[:200]}...\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ No context retrieved - Vector search index might not be set up yet!\")\n",
        "    print(\"\\nðŸ‘‰ Follow the setup instructions below to create the vector index in MongoDB Atlas\")\n",
        "# ============================================================================\n",
        "# MongoDB Atlas Setup Instructions\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âš ï¸  IMPORTANT: MongoDB Atlas Vector Search Index Setup\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "To enable vector search, create an index in MongoDB Atlas:\n",
        "\n",
        "1. Go to MongoDB Atlas Dashboard\n",
        "2. Navigate to your cluster > Browse Collections\n",
        "3. Select database: rag_demo, collection: documents\n",
        "4. Click \"Search Indexes\" tab\n",
        "5. Click \"Create Search Index\"\n",
        "6. Choose \"JSON Editor\"\n",
        "7. Paste this configuration:\n",
        "\n",
        "{\n",
        "  \"fields\": [\n",
        "    {\n",
        "      \"type\": \"vector\",\n",
        "      \"path\": \"embedding\",\n",
        "      \"numDimensions\": 1024,\n",
        "      \"similarity\": \"cosine\"\n",
        "    }\n",
        "  ]\n",
        "}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P4VVlOxfHv4m",
        "outputId": "ea4b15f3-8360-404f-db17-4c894d91848e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "INITIALIZING RAG SYSTEM\n",
            "================================================================================\n",
            "Loading BGE-large model (1024D)...\n",
            "Connecting to MongoDB Atlas...\n",
            "Initializing Gemini model...\n",
            "âœ… RAG System initialized successfully!\n",
            "\n",
            "================================================================================\n",
            "INGESTING DOCUMENT\n",
            "================================================================================\n",
            "Chunking document...\n",
            "Created 1 chunks\n",
            "Generating embeddings and hashes...\n",
            "Chunk 1 already exists (deduplicated)\n",
            "âœ… Document ingestion complete!\n",
            "\n",
            "================================================================================\n",
            "ASKING QUESTION\n",
            "================================================================================\n",
            "ðŸ” Searching for relevant context...\n",
            "ðŸ¤– Generating answer with Gemini...\n",
            "\n",
            "ðŸ“ Question: What is machine learning?\n",
            "\n",
            "âœ… Answer:\n",
            "Machine learning is a subset of AI that enables computers to learn from data without explicit programming.\n",
            "\n",
            "ðŸ“š Used 1 source(s)\n",
            "\n",
            "ðŸ” Retrieved Context:\n",
            "\n",
            "[1] Artificial Intelligence (AI) is transforming the world. Machine learning, a subset of AI, enables computers to learn from data without explicit programming. Deep learning uses neural networks with mul...\n",
            "\n",
            "================================================================================\n",
            "âš ï¸  IMPORTANT: MongoDB Atlas Vector Search Index Setup\n",
            "================================================================================\n",
            "\n",
            "To enable vector search, create an index in MongoDB Atlas:\n",
            "\n",
            "1. Go to MongoDB Atlas Dashboard\n",
            "2. Navigate to your cluster > Browse Collections\n",
            "3. Select database: rag_demo, collection: documents\n",
            "4. Click \"Search Indexes\" tab\n",
            "5. Click \"Create Search Index\"\n",
            "6. Choose \"JSON Editor\"\n",
            "7. Paste this configuration:\n",
            "\n",
            "{\n",
            "  \"fields\": [\n",
            "    {\n",
            "      \"type\": \"vector\",\n",
            "      \"path\": \"embedding\",\n",
            "      \"numDimensions\": 1024,\n",
            "      \"similarity\": \"cosine\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}