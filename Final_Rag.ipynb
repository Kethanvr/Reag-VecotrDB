{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxqxYm+CN9fqFugNIAt9Qk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24a8391241214336b50ad635c56327ab": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8a8bbabf5d3a406ca39b276c7d385369",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32mâ \u001b[0m \u001b[1;32mThinking...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â </span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Thinking...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "8a8bbabf5d3a406ca39b276c7d385369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460f084a252f400e8721b14ebd982a63": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4fbe3c66ff844f5c8fb62ed5091aa491",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32mâ ´\u001b[0m \u001b[1;32mThinking...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â ´</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Thinking...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "4fbe3c66ff844f5c8fb62ed5091aa491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kethanvr/Reag-VecotrDB/blob/main/Final_Rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbgISdWqEo2R",
        "outputId": "d779c7f1-fb10-4eca-8862-f5f2e9712020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m719.1/719.1 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Dependencies installed successfully.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Required Packages\n",
        "!pip install -q pymongo sentence-transformers google-generativeai langchain langchain-google-genai langchain-community pypdf python-docx openpyxl pandas unstructured pillow langchain-text-splitters\n",
        "!pip install -q duckduckgo-search beautifulsoup4 requests\n",
        "!pip install -q rich\n",
        "\n",
        "print(\"âœ… Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEv7RdxWR9lW",
        "outputId": "19305881-b2be-41a3-88af-724b03ebff87"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.12/dist-packages (8.1.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.1)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Load API Keys & Imports\n",
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import uuid\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict, Any, Optional\n",
        "from io import BytesIO\n",
        "\n",
        "# ML & AI Imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from pymongo import MongoClient\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, Docx2txtLoader, TextLoader,\n",
        "    UnstructuredExcelLoader, CSVLoader,\n",
        "    UnstructuredMarkdownLoader, UnstructuredHTMLLoader\n",
        ")\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Web & Search Imports\n",
        "from bs4 import BeautifulSoup\n",
        "from duckduckgo_search import DDGS\n",
        "from PIL import Image\n",
        "\n",
        "# UI Imports\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "\n",
        "# Configuration\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    MONGODB_URI = userdata.get('MONGODB_URI')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    print(\"âœ… API Keys loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading keys: {e}\")\n",
        "    print(\"Please set GEMINI_API_KEY and MONGODB_URI in Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKSVN2hbGRvO",
        "outputId": "e3f77bc4-a975-4a76-a9eb-b0b6407ac451"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API Keys loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Define Advanced RAG Class (Fixed Web Search)\n",
        "import os\n",
        "import hashlib\n",
        "import uuid\n",
        "import time\n",
        "import requests\n",
        "import warnings\n",
        "from typing import List, Dict, Any\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Try to import DDGS, handling potential version issues\n",
        "try:\n",
        "    from duckduckgo_search import DDGS\n",
        "except ImportError:\n",
        "    # Fallback or re-install hint\n",
        "    print(\"âš ï¸ Please run: !pip install -U duckduckgo-search\")\n",
        "\n",
        "# ML & Database Imports\n",
        "import google.generativeai as genai\n",
        "from pymongo import MongoClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, Docx2txtLoader, TextLoader, CSVLoader,\n",
        "    UnstructuredHTMLLoader\n",
        ")\n",
        "\n",
        "class AdvancedRAGWithMemoryVisionWeb:\n",
        "    def __init__(self, mongodb_uri: str, db_name: str, collection_name: str, model_name: str):\n",
        "        print(\"ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\")\n",
        "        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "\n",
        "        print(f\"ğŸ”„ Connecting to MongoDB (DB: {db_name})...\")\n",
        "        self.client = MongoClient(mongodb_uri)\n",
        "        self.db = self.client[db_name]\n",
        "        self.collection = self.db[collection_name]\n",
        "        self.memory_collection = self.db[\"conversation_memory\"]\n",
        "\n",
        "        self.model_name = model_name\n",
        "        print(f\"ğŸ”„ Initializing {self.model_name}...\")\n",
        "        self.llm = genai.GenerativeModel(self.model_name)\n",
        "\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "        self.conversation_history = []\n",
        "        print(\"âœ… System Initialized!\")\n",
        "\n",
        "    # --- MEMORY ---\n",
        "    def add_to_memory(self, role: str, content: str):\n",
        "        msg = {\"role\": role, \"content\": content, \"timestamp\": str(uuid.uuid4())}\n",
        "        self.conversation_history.append(msg)\n",
        "        if self.session_id:\n",
        "            self.memory_collection.insert_one({**msg, \"session_id\": self.session_id})\n",
        "\n",
        "    def get_context_str(self, last_n=5):\n",
        "        return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.conversation_history[-last_n:]])\n",
        "\n",
        "    # --- WEB SEARCH (FIXED) ---\n",
        "    def web_search_and_scrape(self, query: str, max_results=3):\n",
        "        print(f\"ğŸŒ Searching DuckDuckGo for: '{query}'...\")\n",
        "        try:\n",
        "            ddgs = DDGS()\n",
        "            # Use the .text() method which is standard for the new version\n",
        "            results = list(ddgs.text(query, max_results=max_results))\n",
        "\n",
        "            if not results:\n",
        "                return \"\"\n",
        "\n",
        "            context = []\n",
        "            for r in results:\n",
        "                context.append(f\"Source: {r.get('title')}\\nURL: {r.get('href')}\\nSummary: {r.get('body')}\")\n",
        "\n",
        "            return \"\\n\\n\".join(context)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Web Search Error: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    # --- VISION ---\n",
        "    def analyze_uploaded_image(self, uploaded_file_dict, question=\"Describe this image\"):\n",
        "        try:\n",
        "            filename = list(uploaded_file_dict.keys())[0]\n",
        "            image = Image.open(BytesIO(uploaded_file_dict[filename]))\n",
        "            response = self.llm.generate_content([question, image])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error processing image: {str(e)}\"\n",
        "\n",
        "    # --- INGESTION ---\n",
        "    def ingest_file(self, file_path: str, metadata: Dict = None):\n",
        "        ext = os.path.splitext(file_path)[1].lower()\n",
        "        try:\n",
        "            if ext == '.pdf': loader = PyPDFLoader(file_path)\n",
        "            elif ext in ['.docx', '.doc']: loader = Docx2txtLoader(file_path)\n",
        "            elif ext == '.csv': loader = CSVLoader(file_path)\n",
        "            else: loader = TextLoader(file_path)\n",
        "\n",
        "            docs = loader.load()\n",
        "            if not docs: return 0\n",
        "\n",
        "            chunks = self.text_splitter.split_documents(docs)\n",
        "            new_chunks = 0\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_hash = hashlib.sha256(chunk.page_content.encode()).hexdigest()\n",
        "                if self.collection.find_one({\"hash\": chunk_hash}): continue\n",
        "\n",
        "                doc = {\n",
        "                    \"text\": chunk.page_content,\n",
        "                    \"embedding\": self.embedding_model.encode(chunk.page_content).tolist(),\n",
        "                    \"hash\": chunk_hash,\n",
        "                    \"metadata\": {**(metadata or {}), \"chunk_index\": i}\n",
        "                }\n",
        "                self.collection.insert_one(doc)\n",
        "                new_chunks += 1\n",
        "            return new_chunks\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Ingestion Error: {e}\")\n",
        "            return 0\n",
        "\n",
        "    # --- GENERATION ---\n",
        "    def generate_answer(self, query: str):\n",
        "        self.add_to_memory(\"user\", query)\n",
        "\n",
        "        # 1. Search Docs\n",
        "        q_emb = self.embedding_model.encode(query).tolist()\n",
        "        ctx_sources = []\n",
        "        try:\n",
        "            results = list(self.collection.aggregate([\n",
        "                {\"$vectorSearch\": {\n",
        "                    \"index\": \"vector_index\",\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"queryVector\": q_emb,\n",
        "                    \"numCandidates\": 50,\n",
        "                    \"limit\": 3\n",
        "                }},\n",
        "                {\"$project\": {\"_id\": 0, \"text\": 1, \"metadata\": 1, \"score\": {\"$meta\": \"vectorSearchScore\"}}}\n",
        "            ]))\n",
        "            ctx_sources = results\n",
        "        except: pass\n",
        "\n",
        "        # 2. Auto-Web Search\n",
        "        use_web = False\n",
        "        web_content = \"\"\n",
        "        highest_score = ctx_sources[0]['score'] if ctx_sources else 0\n",
        "\n",
        "        # If relevance is low (< 0.5) OR no docs found, trigger web search\n",
        "        if not ctx_sources or highest_score < 0.5:\n",
        "            use_web = True\n",
        "            web_content = self.web_search_and_scrape(query)\n",
        "\n",
        "        # 3. Prompt\n",
        "        doc_text = \"\\n\\n\".join([f\"[Doc: {r['metadata'].get('source')}]\\n{r['text']}\" for r in ctx_sources])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Answer the user's question.\n",
        "        - If the document context is relevant, use it.\n",
        "        - If the document context is missing or irrelevant, use the Web Context.\n",
        "\n",
        "        Context from Documents:\n",
        "        {doc_text}\n",
        "\n",
        "        Context from Web Search:\n",
        "        {web_content}\n",
        "\n",
        "        User Question: {query}\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.llm.generate_content(prompt)\n",
        "        answer = response.text\n",
        "        self.add_to_memory(\"assistant\", answer)\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": ctx_sources,\n",
        "            \"web_used\": bool(web_content)\n",
        "        }\n",
        "\n",
        "# --- INITIALIZATION ---\n",
        "try: ENV_DB = userdata.get('DB_NAME')\n",
        "except: ENV_DB = \"rag\"\n",
        "try: ENV_COLLECTION = userdata.get('COLLECTION_NAME')\n",
        "except: ENV_COLLECTION = \"rag-collection\"\n",
        "try: ENV_MODEL = userdata.get('GEMINI_MODEL')\n",
        "except: ENV_MODEL = \"gemini-2.0-flash\"\n",
        "\n",
        "rag = AdvancedRAGWithMemoryVisionWeb(\n",
        "    mongodb_uri=userdata.get('MONGODB_URI'),\n",
        "    db_name=ENV_DB,\n",
        "    collection_name=ENV_COLLECTION,\n",
        "    model_name=ENV_MODEL\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV4iqpIMGaaD",
        "outputId": "46cb91b6-8819-46e7-e762-7a58b561c1ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\n",
            "ğŸ”„ Connecting to MongoDB (DB: rag\n",
            ")...\n",
            "ğŸ”„ Initializing gemini-2.5-flash-lite...\n",
            "âœ… System Initialized!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3807184204.py:191: ResourceWarning: Unclosed MongoClient opened at:\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-80821351.py\", line 3, in <cell line: 0>\n",
            "    rag = AdvancedRAGWithMemoryVisionWeb(\n",
            "  File \"/tmp/ipython-input-131372669.py\", line 35, in __init__\n",
            "    self.client = MongoClient(mongodb_uri)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/synchronous/mongo_client.py\", line 891, in __init__\n",
            "    self._get_topology()  # type: ignore[unused-coroutine]\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/synchronous/mongo_client.py\", line 1758, in _get_topology\n",
            "    self._resolve_srv()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/synchronous/mongo_client.py\", line 960, in _resolve_srv\n",
            "    self._init_based_on_options(seeds, srv_max_hosts, srv_service_name)\n",
            "Call MongoClient.close() to safely shut down your client and free up resources.\n",
            "  rag = AdvancedRAGWithMemoryVisionWeb(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Initialize & Create Indexes\n",
        "# Initialize\n",
        "rag = AdvancedRAGWithMemoryVisionWeb(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    db_name=\"rag\",\n",
        "    collection_name=\"rag-collection\"\n",
        ")\n",
        "\n",
        "# Create Hash Index for Deduplication (Runs once)\n",
        "print(\"ğŸ”§ Ensuring hash index exists...\")\n",
        "try:\n",
        "    rag.collection.create_index(\"hash\", unique=True)\n",
        "    print(\"âœ… Hash index verified!\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "\n",
        "rag.start_new_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "drXHHOKWTGiQ",
        "outputId": "c243f26c-3f61-4488-84db-e9a2c75e6fba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading Embedding model (BAAI/bge-large-en-v1.5)...\n",
            "ğŸ”„ Connecting to MongoDB Atlas...\n",
            "ğŸ”„ Initializing gemini-2.5-flash-lite...\n",
            "âœ… System Initialized!\n",
            "ğŸ”§ Ensuring hash index exists...\n",
            "âœ… Hash index verified!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'afb2bd9d-987c-42bd-9f6d-9dd30534339b'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ynUjAgn-tV1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Upload & Ingest Documents (FIXED)\n",
        "print(\"ğŸ“¤ Upload documents (PDF, DOCX, TXT, CSV)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "total_chunks = 0\n",
        "\n",
        "for filename, content in uploaded.items():\n",
        "    print(f\"Processing {filename}...\")\n",
        "\n",
        "    # Write to temp file\n",
        "    temp_path = f\"/tmp/{filename}\"\n",
        "    with open(temp_path, 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "    # Ingest\n",
        "    chunks_added = rag.ingest_file(temp_path, metadata={\"source\": filename})\n",
        "\n",
        "    # Safety Check: Ensure chunks_added is a number before adding\n",
        "    if chunks_added is not None:\n",
        "        total_chunks += chunks_added\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: No chunks returned for {filename}\")\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n",
        "\n",
        "print(f\"\\nâœ… Ingestion Complete! Added {total_chunks} new chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "himZqPYXmfuW",
        "outputId": "c8a3e6c7-90ea-4b3d-faec-1f56cb3910f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¤ Upload documents (PDF, DOCX, TXT, CSV)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ffb4dc59-26c0-4f96-9d46-b70fb6c362bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ffb4dc59-26c0-4f96-9d46-b70fb6c362bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Kethan VR A Comprehensive.md to Kethan VR A Comprehensive.md\n",
            "Processing Kethan VR A Comprehensive.md...\n",
            "\n",
            "âœ… Ingestion Complete! Added 0 new chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Interactive Chat Interface (Zero-Noise Mode)\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# --- NUCLEAR OPTION: SUPPRESS ALL NOISE ---\n",
        "# 1. Standard Python Warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "\n",
        "# 2. Silence Third-Party Loggers\n",
        "logging.getLogger(\"tornado.access\").setLevel(logging.CRITICAL)\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.CRITICAL)\n",
        "logging.getLogger(\"duckduckgo_search\").setLevel(logging.CRITICAL)\n",
        "\n",
        "# 3. Context Manager to eat stderr during noisy operations\n",
        "@contextmanager\n",
        "def suppress_stderr():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stderr = sys.stderr\n",
        "        sys.stderr = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stderr = old_stderr\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "from google.colab import files\n",
        "\n",
        "console = Console()\n",
        "\n",
        "console.print(Panel.fit(\n",
        "    \"[bold cyan]Advanced RAG System[/bold cyan]\\n\"\n",
        "    \"[dim]Commands:[/dim]\\n\"\n",
        "    \"â€¢ [green]Any question[/green]: Search docs + memory\\n\"\n",
        "    \"â€¢ [green]web: <query>[/green]: Force web search\\n\"\n",
        "    \"â€¢ [green]image[/green]: Upload and analyze image\\n\"\n",
        "    \"â€¢ [green]exit[/green]: Quit\",\n",
        "    title=\"ğŸš€ Ready (Zero-Noise Mode)\", border_style=\"cyan\"\n",
        "))\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # Use standard input() to avoid Colab recursion errors\n",
        "        print(\"\\n\" + \"â”€\"*50)\n",
        "        query = input(\"User ğŸ‘¤ (Type 'exit' to quit): \").strip()\n",
        "\n",
        "        if query.lower() in ['exit', 'quit', 'q']:\n",
        "            console.print(\"[bold red]ğŸ‘‹ Goodbye![/bold red]\")\n",
        "            break\n",
        "\n",
        "        if not query: continue\n",
        "\n",
        "        # --- IMAGE MODE ---\n",
        "        if query.lower() == 'image':\n",
        "            console.print(\"[yellow]ğŸ“¤ Upload an image...[/yellow]\")\n",
        "            img_upload = files.upload()\n",
        "            if img_upload:\n",
        "                q_img = input(\"Question about image: \")\n",
        "                with console.status(\"[bold green]Analyzing Image...[/bold green]\"):\n",
        "                    with suppress_stderr(): # SILENCE WARNINGS HERE\n",
        "                        ans = rag.analyze_uploaded_image(img_upload, q_img)\n",
        "                console.print(Panel(Markdown(ans), title=\"ğŸ–¼ï¸ Image Analysis\", border_style=\"magenta\"))\n",
        "            continue\n",
        "\n",
        "        # --- TEXT/WEB MODE ---\n",
        "        use_web = False\n",
        "        if query.startswith(\"web:\"):\n",
        "            use_web = True\n",
        "            query = query.replace(\"web:\", \"\").strip()\n",
        "\n",
        "        with console.status(\"[bold green]Thinking...[/bold green]\"):\n",
        "            try:\n",
        "                # WRAP GENERATION IN SILENCE\n",
        "                # This eats the \"duckduckgo\" and \"ipywidgets\" red text\n",
        "                with suppress_stderr():\n",
        "                    if hasattr(rag, 'web_search_and_scrape'):\n",
        "                         result = rag.generate_answer(query)\n",
        "                    else:\n",
        "                         result = rag.generate_answer(query)\n",
        "\n",
        "                # Display Answer\n",
        "                console.print(Panel(\n",
        "                    Markdown(result['answer']),\n",
        "                    title=\"ğŸ¤– AI Response\",\n",
        "                    border_style=\"green\",\n",
        "                    box=box.ROUNDED\n",
        "                ))\n",
        "\n",
        "                # Display Sources\n",
        "                if result.get('sources'):\n",
        "                    table = Table(title=\"ğŸ“š Sources Used\", box=box.SIMPLE)\n",
        "                    table.add_column(\"Score\", style=\"cyan\")\n",
        "                    table.add_column(\"Source File\", style=\"magenta\")\n",
        "\n",
        "                    for s in result['sources']:\n",
        "                        table.add_row(\n",
        "                            f\"{s.get('score', 0):.2f}\",\n",
        "                            s['metadata'].get('source', 'unknown')\n",
        "                        )\n",
        "                    console.print(table)\n",
        "\n",
        "                if result.get('web_used'):\n",
        "                    console.print(\"[dim]ğŸŒ Web content was used to answer this.[/dim]\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # We turn stderr back on for actual errors so you can see them\n",
        "                console.print(f\"[red]Generation Error: {e}[/red]\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        console.print(\"\\n[bold red]ğŸ‘‹ Goodbye![/bold red]\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        console.print(f\"[red]System Error: {e}[/red]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632,
          "referenced_widgets": [
            "24a8391241214336b50ad635c56327ab",
            "8a8bbabf5d3a406ca39b276c7d385369",
            "460f084a252f400e8721b14ebd982a63",
            "4fbe3c66ff844f5c8fb62ed5091aa491"
          ]
        },
        "id": "hml6bMSzQVhX",
        "outputId": "19c71742-894b-4a6c-923b-4563519bbb81"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€\u001b[0m\u001b[36m ğŸš€ Ready (Zero-Noise Mode) \u001b[0m\u001b[36mâ”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[1;36mAdvanced RAG System\u001b[0m                  \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[2mCommands:\u001b[0m                            \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mAny question\u001b[0m: Search docs + memory \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mweb: <query>\u001b[0m: Force web search     \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mimage\u001b[0m: Upload and analyze image    \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m â€¢ \u001b[32mexit\u001b[0m: Quit                         \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€ ğŸš€ Ready (Zero-Noise Mode) â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Advanced RAG System</span>                  <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Commands:</span>                            <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">Any question</span>: Search docs + memory <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">web: &lt;query&gt;</span>: Force web search     <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">image</span>: Upload and analyze image    <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> â€¢ <span style=\"color: #008000; text-decoration-color: #008000\">exit</span>: Quit                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "User ğŸ‘¤ (Type 'exit' to quit): who is modi\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24a8391241214336b50ad635c56327ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸŒ Searching DuckDuckGo for: 'who is modi'...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸŒ Searching DuckDuckGo for: 'who is modi'...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[32mâ•­â”€\u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32m ğŸ¤– AI Response \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m Narendra Modi is the current Prime Minister of India. He assumed office on May 26, 2014, and is a prominent     \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ”‚\u001b[0m leader of the Bharatiya Janata Party (BJP).                                                                     \u001b[32mâ”‚\u001b[0m\n",
              "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¤– AI Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> Narendra Modi is the current Prime Minister of India. He assumed office on May 26, 2014, and is a prominent     <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> leader of the Bharatiya Janata Party (BJP).                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7970fe273a10>\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "User ğŸ‘¤ (Type 'exit' to quit): kethan?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "460f084a252f400e8721b14ebd982a63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸŒ Searching DuckDuckGo for: 'kethan?'...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸŒ Searching DuckDuckGo for: 'kethan?'...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31mGeneration Error: \u001b[0m\u001b[1;31m429\u001b[0m\u001b[31m POST \u001b[0m\n",
              "\u001b[4;31mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?%\u001b[0m\u001b[4;31m24alt\u001b[0m\u001b[4;31m=\u001b[0m\u001b[4;31mjson\u001b[0m\u001b[4;31m%3Benum-en\u001b[0m\n",
              "\u001b[4;31mcoding%3Dint:\u001b[0m\u001b[31m You exceeded your current quota, please check your plan and billing details. For more information on \u001b[0m\n",
              "\u001b[31mthis error, head to: \u001b[0m\u001b[4;31mhttps://ai.google.dev/gemini-api/docs/rate-limits.\u001b[0m\u001b[31m To monitor your current usage, head to: \u001b[0m\n",
              "\u001b[4;31mhttps://ai.dev/rate-limit.\u001b[0m\u001b[31m \u001b[0m\n",
              "\u001b[31m* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: \u001b[0m\u001b[1;31m20\u001b[0m\u001b[31m, \u001b[0m\n",
              "\u001b[31mmodel: gemini-\u001b[0m\u001b[1;31m2.5\u001b[0m\u001b[31m-flash-lite\u001b[0m\n",
              "\u001b[31mPlease retry in \u001b[0m\u001b[1;31m24.\u001b[0m\u001b[31m506032167s.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Generation Error: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">429</span><span style=\"color: #800000; text-decoration-color: #800000\"> POST </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; text-decoration: underline\">https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?%24alt=json%3Benum-en</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; text-decoration: underline\">coding%3Dint:</span><span style=\"color: #800000; text-decoration-color: #800000\"> You exceeded your current quota, please check your plan and billing details. For more information on </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">this error, head to: </span><span style=\"color: #800000; text-decoration-color: #800000; text-decoration: underline\">https://ai.google.dev/gemini-api/docs/rate-limits.</span><span style=\"color: #800000; text-decoration-color: #800000\"> To monitor your current usage, head to: </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; text-decoration: underline\">https://ai.dev/rate-limit.</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">20</span><span style=\"color: #800000; text-decoration-color: #800000\">, </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">model: gemini-</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2.5</span><span style=\"color: #800000; text-decoration-color: #800000\">-flash-lite</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">Please retry in </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">24.</span><span style=\"color: #800000; text-decoration-color: #800000\">506032167s.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7970fe273a80>\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;31mğŸ‘‹ Goodbye!\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ğŸ‘‹ Goodbye!</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}