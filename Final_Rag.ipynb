{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6dmrnWXYoh5FlgfevI0Ii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kethanvr/Reag-VecotrDB/blob/main/Final_Rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "CbgISdWqEo2R"
      },
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "!pip install -q pymongo sentence-transformers google-generativeai langchain langchain-google-genai langchain-community pypdf python-docx openpyxl pandas unstructured pillow langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from pymongo import MongoClient\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    Docx2txtLoader,\n",
        "    TextLoader,\n",
        "    UnstructuredExcelLoader,\n",
        "    CSVLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    UnstructuredHTMLLoader\n",
        ")\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# For file uploads\n",
        "from google.colab import files\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "cKSVN2hbGRvO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "920b78c6"
      },
      "source": [
        "import os\n",
        "import hashlib\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from pymongo import MongoClient\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    Docx2txtLoader,\n",
        "    TextLoader,\n",
        "    UnstructuredExcelLoader,\n",
        "    CSVLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    UnstructuredHTMLLoader\n",
        ")\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# For file uploads\n",
        "from google.colab import files\n",
        "import tempfile"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get API keys from Colab Secrets\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "MONGODB_URI = userdata.get('MONGODB_URI')\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "print(\"âœ… Configuration loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV4iqpIMGaaD",
        "outputId": "8f2d8617-ca07-48e0-8b18-ac727c42547d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Configuration loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedRAGSystem:\n",
        "    def __init__(self, mongodb_uri: str, db_name: str = \"rag\", collection_name: str = \"documents\"):\n",
        "        \"\"\"Initialize Advanced RAG system with multi-format support\"\"\"\n",
        "\n",
        "        # 1. Load BGE embedding model (1024 dimensions)\n",
        "        print(\"ğŸ”„ Loading BGE-large model (1024D)...\")\n",
        "        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "        self.embedding_dim = 1024\n",
        "\n",
        "        # 2. Initialize MongoDB connection\n",
        "        print(\"ğŸ”„ Connecting to MongoDB Atlas...\")\n",
        "        self.client = MongoClient(mongodb_uri)\n",
        "        self.db = self.client[db_name]\n",
        "        self.collection = self.db[collection_name]\n",
        "\n",
        "        # 3. Initialize Gemini model via LangChain\n",
        "        print(\"ğŸ”„ Initializing Gemini model...\")\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-2.5-flash-lite\",\n",
        "            google_api_key=GEMINI_API_KEY,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # 4. Initialize text splitter (LangChain)\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        print(\"âœ… Advanced RAG System initialized successfully!\")\n",
        "\n",
        "    def load_file(self, file_path: str) -> List[Document]:\n",
        "        \"\"\"Load any file format using LangChain loaders\"\"\"\n",
        "\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        print(f\"ğŸ“„ Loading {file_extension} file...\")\n",
        "\n",
        "        try:\n",
        "            if file_extension == '.pdf':\n",
        "                loader = PyPDFLoader(file_path)\n",
        "            elif file_extension in ['.docx', '.doc']:\n",
        "                loader = Docx2txtLoader(file_path)\n",
        "            elif file_extension == '.txt':\n",
        "                loader = TextLoader(file_path)\n",
        "            elif file_extension in ['.xlsx', '.xls']:\n",
        "                loader = UnstructuredExcelLoader(file_path)\n",
        "            elif file_extension == '.csv':\n",
        "                loader = CSVLoader(file_path)\n",
        "            elif file_extension == '.md':\n",
        "                loader = UnstructuredMarkdownLoader(file_path)\n",
        "            elif file_extension in ['.html', '.htm']:\n",
        "                loader = UnstructuredHTMLLoader(file_path)\n",
        "            else:\n",
        "                print(f\"âš ï¸ Unsupported file format: {file_extension}\")\n",
        "                return []\n",
        "\n",
        "            documents = loader.load()\n",
        "            print(f\"âœ… Loaded {len(documents)} page(s)/section(s)\")\n",
        "            return documents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading file: {e}\")\n",
        "            return []\n",
        "\n",
        "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks using LangChain\"\"\"\n",
        "        print(\"ğŸ”„ Splitting documents into chunks...\")\n",
        "        chunks = self.text_splitter.split_documents(documents)\n",
        "        print(f\"âœ… Created {len(chunks)} chunks\")\n",
        "        return chunks\n",
        "\n",
        "    def generate_hash(self, text: str) -> str:\n",
        "        \"\"\"Generate SHA-256 hash for deduplication\"\"\"\n",
        "        return hashlib.sha256(text.encode()).hexdigest()\n",
        "\n",
        "    def ingest_file(self, file_path: str, metadata: Dict[str, Any] = None):\n",
        "        \"\"\"Ingest any file: load, chunk, embed, and store\"\"\"\n",
        "        if metadata is None:\n",
        "            metadata = {}\n",
        "\n",
        "        # Step 1: Load the file\n",
        "        documents = self.load_file(file_path)\n",
        "        if not documents:\n",
        "            print(\"âŒ No documents loaded. Aborting ingestion.\")\n",
        "            return\n",
        "\n",
        "        # Step 2: Chunk the documents\n",
        "        chunks = self.chunk_documents(documents)\n",
        "\n",
        "        # Step 3: Generate embeddings and store\n",
        "        print(\"ğŸ”„ Generating embeddings and storing in MongoDB...\")\n",
        "        stored_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            # Generate embedding (1024D vector)\n",
        "            embedding = self.embedding_model.encode(chunk.page_content).tolist()\n",
        "\n",
        "            # Generate dedup hash\n",
        "            chunk_hash = self.generate_hash(chunk.page_content)\n",
        "\n",
        "            # Check if chunk already exists (deduplication)\n",
        "            existing = self.collection.find_one({\"hash\": chunk_hash})\n",
        "            if existing:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Merge metadata\n",
        "            chunk_metadata = {**metadata, **chunk.metadata, \"chunk_index\": i}\n",
        "\n",
        "            # Store in MongoDB\n",
        "            document = {\n",
        "                \"text\": chunk.page_content,\n",
        "                \"embedding\": embedding,\n",
        "                \"hash\": chunk_hash,\n",
        "                \"metadata\": chunk_metadata\n",
        "            }\n",
        "\n",
        "            self.collection.insert_one(document)\n",
        "            stored_count += 1\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"  âœ“ Processed {i + 1}/{len(chunks)} chunks...\")\n",
        "\n",
        "        print(f\"âœ… Ingestion complete! Stored: {stored_count}, Skipped (duplicates): {skipped_count}\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Perform semantic search using vector similarity\"\"\"\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_model.encode(query).tolist()\n",
        "\n",
        "        # MongoDB Vector Search Pipeline\n",
        "        pipeline = [\n",
        "            {\n",
        "                \"$vectorSearch\": {\n",
        "                    \"index\": \"vector_index\",\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"queryVector\": query_embedding,\n",
        "                    \"numCandidates\": top_k * 10,\n",
        "                    \"limit\": top_k\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"$project\": {\n",
        "                    \"_id\": 0,\n",
        "                    \"text\": 1,\n",
        "                    \"metadata\": 1,\n",
        "                    \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            results = list(self.collection.aggregate(pipeline))\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Search error: {e}\")\n",
        "            print(\"ğŸ’¡ Make sure vector search index 'vector_index' is created!\")\n",
        "            return []\n",
        "\n",
        "    def generate_answer(self, query: str, top_k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Generate answer using RAG approach with LangChain\"\"\"\n",
        "        # Step 1: Retrieve relevant context\n",
        "        print(f\"ğŸ” Searching for relevant context...\")\n",
        "        search_results = self.search(query, top_k=top_k)\n",
        "\n",
        "        if not search_results:\n",
        "            return {\n",
        "                \"answer\": \"âš ï¸ I couldn't find relevant information. Please ensure:\\n1. Documents are uploaded\\n2. Vector search index is created in MongoDB Atlas\",\n",
        "                \"sources\": [],\n",
        "                \"context_used\": [],\n",
        "                \"num_sources\": 0\n",
        "            }\n",
        "\n",
        "        # Step 2: Prepare context\n",
        "        context_chunks = [result['text'] for result in search_results]\n",
        "        context = \"\\n\\n\".join([f\"[Source {i+1}]\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
        "\n",
        "        # Step 3: Create prompt\n",
        "        prompt = f\"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\n",
        "If the context doesn't contain enough information, clearly state that.\n",
        "Do NOT make up information or use external knowledge.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Provide a clear, concise answer:\"\"\"\n",
        "\n",
        "        # Step 4: Generate answer using Gemini via LangChain\n",
        "        print(\"ğŸ¤– Generating answer with Gemini...\")\n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            answer = response.content\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error generating answer: {e}\")\n",
        "            answer = \"Sorry, I encountered an error while generating the answer.\"\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": search_results,\n",
        "            \"context_used\": context_chunks,\n",
        "            \"num_sources\": len(search_results)\n",
        "        }\n",
        "\n",
        "print(\"âœ… AdvancedRAGSystem class defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWvNksumG-Dn",
        "outputId": "82097a86-ad61-4214-b3c8-07f5222e746b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… AdvancedRAGSystem class defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸš€ INITIALIZING ADVANCED RAG SYSTEM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rag = AdvancedRAGSystem(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    db_name=\"rag\",\n",
        "    collection_name=\"rag-collection\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKMtPwH5KU6k",
        "outputId": "2b9c1ad9-bf97-4a95-bd9a-2976a05bb446"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸš€ INITIALIZING ADVANCED RAG SYSTEM\n",
            "================================================================================\n",
            "ğŸ”„ Loading BGE-large model (1024D)...\n",
            "ğŸ”„ Connecting to MongoDB Atlas...\n",
            "ğŸ”„ Initializing Gemini model...\n",
            "âœ… Advanced RAG System initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“¤ UPLOAD YOUR DOCUMENTS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "Supported formats:\n",
        "âœ“ PDF (.pdf)\n",
        "âœ“ Word (.docx, .doc)\n",
        "âœ“ Text (.txt)\n",
        "âœ“ Excel (.xlsx, .xls)\n",
        "âœ“ CSV (.csv)\n",
        "âœ“ Markdown (.md)\n",
        "âœ“ HTML (.html, .htm)\n",
        "\n",
        "ğŸ”’ Deduplication: Enabled (SHA-256 hashing)\n",
        "   â†’ Duplicate chunks will be automatically skipped\n",
        "\"\"\")\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Track statistics\n",
        "total_stored = 0\n",
        "total_skipped = 0\n",
        "\n",
        "# Process each uploaded file\n",
        "for filename, file_content in uploaded.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ğŸ“„ Processing: {filename}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Save to temporary file\n",
        "    temp_path = f\"/tmp/{filename}\"\n",
        "    with open(temp_path, 'wb') as f:\n",
        "        f.write(file_content)\n",
        "\n",
        "    # Ingest the file\n",
        "    rag.ingest_file(\n",
        "        file_path=temp_path,\n",
        "        metadata={\n",
        "            \"source\": filename,\n",
        "            \"file_type\": os.path.splitext(filename)[1]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Clean up\n",
        "    os.remove(temp_path)\n",
        "    print(f\"âœ… {filename} processed\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š INGESTION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\"\"\n",
        "Total files processed: {len(uploaded)}\n",
        "âœ… Stored new chunks: Check MongoDB stats\n",
        "ğŸ”’ Skipped duplicates: Check individual file outputs above\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "P4VVlOxfHv4m",
        "outputId": "f10324f6-77c2-49a8-e811-0570bd5ce0bc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ“¤ UPLOAD YOUR DOCUMENTS\n",
            "================================================================================\n",
            "\n",
            "Supported formats:\n",
            "âœ“ PDF (.pdf)\n",
            "âœ“ Word (.docx, .doc)\n",
            "âœ“ Text (.txt)\n",
            "âœ“ Excel (.xlsx, .xls)\n",
            "âœ“ CSV (.csv)\n",
            "âœ“ Markdown (.md)\n",
            "âœ“ HTML (.html, .htm)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-88c4c33a-f0a5-4895-b9be-4cf0199dc198\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-88c4c33a-f0a5-4895-b9be-4cf0199dc198\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Kethan VR A Comprehensive.md to Kethan VR A Comprehensive (1).md\n",
            "\n",
            "ğŸ“„ Processing: Kethan VR A Comprehensive (1).md\n",
            "ğŸ“„ Loading .md file...\n",
            "âœ… Loaded 1 page(s)/section(s)\n",
            "ğŸ”„ Splitting documents into chunks...\n",
            "âœ… Created 24 chunks\n",
            "ğŸ”„ Generating embeddings and storing in MongoDB...\n",
            "âœ… Ingestion complete! Stored: 0, Skipped (duplicates): 24\n",
            "\n",
            "âœ… All files processed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this ONCE to create hash index\n",
        "print(\"ğŸ”§ Creating hash index for faster deduplication...\")\n",
        "rag.collection.create_index(\"hash\", unique=True)\n",
        "print(\"âœ… Hash index created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drXHHOKWTGiQ",
        "outputId": "b64a7b2a-5dd4-46f8-8437-e52bdc8f7327"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Creating hash index for faster deduplication...\n",
            "âœ… Hash index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First install rich for beautiful formatting\n",
        "!pip install -q rich\n"
      ],
      "metadata": {
        "id": "CTij0VHJTbhn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Then use this enhanced version:\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.table import Table\n",
        "from rich import box\n",
        "\n",
        "console = Console()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "console.print(\"ğŸ’¬ [bold cyan]INTERACTIVE Q&A SYSTEM[/bold cyan]\")\n",
        "print(\"=\"*80)\n",
        "console.print(\"[yellow]Ask questions about your uploaded documents![/yellow]\")\n",
        "console.print(\"[dim]Type 'exit' or 'quit' to stop.[/dim]\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user question\n",
        "    question = console.input(\"[bold green]â“ Your question:[/bold green] \").strip()\n",
        "\n",
        "    # Exit condition\n",
        "    if question.lower() in ['exit', 'quit', 'q', '']:\n",
        "        console.print(\"\\n[bold]ğŸ‘‹ Goodbye![/bold]\")\n",
        "        break\n",
        "\n",
        "    console.print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    # Generate answer\n",
        "    result = rag.generate_answer(question, top_k=3)\n",
        "\n",
        "    # ========================================================================\n",
        "    # FORMATTED ANSWER OUTPUT WITH RICH\n",
        "    # ========================================================================\n",
        "\n",
        "    answer = result.get('answer', 'No answer generated')\n",
        "\n",
        "    # Display answer in a panel\n",
        "    console.print(Panel(\n",
        "        Markdown(answer),\n",
        "        title=\"ğŸ¤– Answer\",\n",
        "        border_style=\"cyan\",\n",
        "        box=box.ROUNDED\n",
        "    ))\n",
        "\n",
        "    # ========================================================================\n",
        "    # SOURCE INFORMATION TABLE\n",
        "    # ========================================================================\n",
        "\n",
        "    num_sources = result.get('num_sources', 0)\n",
        "    sources = result.get('sources', [])\n",
        "\n",
        "    if num_sources > 0:\n",
        "        # Create sources table\n",
        "        table = Table(title=f\"ğŸ“š Sources ({num_sources} found)\", box=box.ROUNDED)\n",
        "        table.add_column(\"#\", style=\"cyan\", width=4)\n",
        "        table.add_column(\"File\", style=\"green\")\n",
        "        table.add_column(\"Page/Section\", style=\"yellow\")\n",
        "        table.add_column(\"Relevance\", style=\"magenta\")\n",
        "\n",
        "        for i, source in enumerate(sources, 1):\n",
        "            metadata = source.get('metadata', {})\n",
        "            score = source.get('score', 0)\n",
        "\n",
        "            table.add_row(\n",
        "                str(i),\n",
        "                metadata.get('source', 'Unknown'),\n",
        "                str(metadata.get('page', 'N/A')),\n",
        "                f\"{score:.4f}\"\n",
        "            )\n",
        "\n",
        "        console.print(table)\n",
        "\n",
        "        # ====================================================================\n",
        "        # OPTION TO VIEW FULL CONTEXT\n",
        "        # ====================================================================\n",
        "\n",
        "        console.print(\"\\n[bold]Options:[/bold]\")\n",
        "        console.print(\"  [1] Show all source context\")\n",
        "        console.print(\"  [2] Show specific source\")\n",
        "        console.print(\"  [3] Continue to next question\")\n",
        "\n",
        "        choice = console.input(\"\\n[bold yellow]Select option (1/2/3):[/bold yellow] \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            # Show all context\n",
        "            context_used = result.get('context_used', [])\n",
        "            for i, context in enumerate(context_used, 1):\n",
        "                console.print(Panel(\n",
        "                    context,\n",
        "                    title=f\"ğŸ“– Source {i} - Full Context\",\n",
        "                    border_style=\"blue\",\n",
        "                    box=box.ROUNDED\n",
        "                ))\n",
        "\n",
        "        elif choice == '2':\n",
        "            # Show specific source\n",
        "            source_num = console.input(f\"\\n[bold]Which source? (1-{num_sources}):[/bold] \").strip()\n",
        "            try:\n",
        "                idx = int(source_num) - 1\n",
        "                if 0 <= idx < num_sources:\n",
        "                    context_used = result.get('context_used', [])\n",
        "                    console.print(Panel(\n",
        "                        context_used[idx],\n",
        "                        title=f\"ğŸ“– Source {source_num} - Full Context\",\n",
        "                        border_style=\"green\",\n",
        "                        box=box.ROUNDED\n",
        "                    ))\n",
        "                else:\n",
        "                    console.print(\"[red]âš ï¸ Invalid source number[/red]\")\n",
        "            except ValueError:\n",
        "                console.print(\"[red]âš ï¸ Please enter a valid number[/red]\")\n",
        "\n",
        "    else:\n",
        "        console.print(Panel(\n",
        "            \"[yellow]Make sure:\\n1. Documents are uploaded\\n2. Vector search index is created\\n3. Question relates to uploaded content[/yellow]\",\n",
        "            title=\"âš ï¸ No Sources Found\",\n",
        "            border_style=\"red\",\n",
        "            box=box.ROUNDED\n",
        "        ))\n",
        "\n",
        "    console.print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YmxZkYeNPjTj",
        "outputId": "890880c7-538e-4544-9bc6-d73637271636"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ğŸ’¬ \u001b[1;36mINTERACTIVE Q&A SYSTEM\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ’¬ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">INTERACTIVE Q&amp;A SYSTEM</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[33mAsk questions about your uploaded documents!\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Ask questions about your uploaded documents!</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mType \u001b[0m\u001b[2;32m'exit'\u001b[0m\u001b[2m or \u001b[0m\u001b[2;32m'quit'\u001b[0m\u001b[2m to stop.\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Type </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'exit'</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> or </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'quit'</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> to stop.</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mâ“ Your question:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â“ Your question:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "who is kethan vr and give is social media links\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "================================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "================================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Searching for relevant context...\n",
            "ğŸ¤– Generating answer with Gemini...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m ğŸ¤– Answer \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m Kethan VR is an emerging tech innovator and an early-stage developer and engineering student in India. He is    \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m described as an AI-first builder who has built a fullstack portfolio and shipped multiple AI-integrated         \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m projects.                                                                                                       \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m His social media links are:                                                                                     \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[1;33m â€¢ \u001b[0m\u001b[1mGitHub:\u001b[0m github.com/Kethanvr                                                                                  \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[1;33m â€¢ \u001b[0m\u001b[1mLinkedIn:\u001b[0m linkedin.com/in/kethan-vr                                                                          \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[1;33m â€¢ \u001b[0m\u001b[1mYouTube:\u001b[0m @kethanvr                                                                                           \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ”‚\u001b[0m \u001b[1;33m â€¢ \u001b[0m\u001b[1mTwitter/X:\u001b[0m x.com/VrKethan                                                                                    \u001b[36mâ”‚\u001b[0m\n",
              "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¤– Answer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> Kethan VR is an emerging tech innovator and an early-stage developer and engineering student in India. He is    <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> described as an AI-first builder who has built a fullstack portfolio and shipped multiple AI-integrated         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> projects.                                                                                                       <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> His social media links are:                                                                                     <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">GitHub:</span> github.com/Kethanvr                                                                                  <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">LinkedIn:</span> linkedin.com/in/kethan-vr                                                                          <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">YouTube:</span> @kethanvr                                                                                           <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Twitter/X:</span> x.com/VrKethan                                                                                    <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                         ğŸ“š Sources (3 found)                          \u001b[0m\n",
              "â•­â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚\u001b[1m \u001b[0m\u001b[1m#   \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mFile                             \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mPage/Section\u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mRelevance\u001b[0m\u001b[1m \u001b[0mâ”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚\u001b[36m \u001b[0m\u001b[36m1   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mN/A         \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m0.8898   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
              "â”‚\u001b[36m \u001b[0m\u001b[36m2   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mN/A         \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m0.8781   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
              "â”‚\u001b[36m \u001b[0m\u001b[36m3   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m/tmp/Kethan VR A Comprehensive.md\u001b[0m\u001b[32m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mN/A         \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m0.8598   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                         ğŸ“š Sources (3 found)                          </span>\n",
              "â•­â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚<span style=\"font-weight: bold\"> #    </span>â”‚<span style=\"font-weight: bold\"> File                              </span>â”‚<span style=\"font-weight: bold\"> Page/Section </span>â”‚<span style=\"font-weight: bold\"> Relevance </span>â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> 1    </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> /tmp/Kethan VR A Comprehensive.md </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> N/A          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> 0.8898    </span>â”‚\n",
              "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> 2    </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> /tmp/Kethan VR A Comprehensive.md </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> N/A          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> 0.8781    </span>â”‚\n",
              "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> 3    </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> /tmp/Kethan VR A Comprehensive.md </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> N/A          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> 0.8598    </span>â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1mOptions:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\">Options:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m Show all source context\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span> Show all source context\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m Show specific source\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span> Show specific source\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  \u001b[1m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m Continue to next question\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span> Continue to next question\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33mSelect option \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Select option (</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">):</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[34mâ•­â”€\u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34m ğŸ“– Source 1 - Full Context \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m SEO/Organic: If he's ranking for \"Kethan VR,\" he's already thinking about personal branding and discoverability \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m What's missing (but could be added): - Public GitHub projects with 100+ stars (but this could come with scaled  \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m projects) - Shipped product with paying users (but he's 19 and in collegeâ€”time will tell) - Case studies        \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m explaining the problem-solution fit for each project                                                            \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Part 3: Online Presence & Personal Branding                                                                     \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Multi-Platform Strategy                                                                                         \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Platform URL Strategy Portfolio kethanvr.me Central hub; SEO/discovery GitHub github.com/Kethanvr Code quality  \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m and contributions LinkedIn linkedin.com/in/kethan-vr Professional network, recruiter reach YouTube @kethanvr    \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Long-form tutorials, thought leadership Twitter/X x.com/VrKethan Real-time updates, community engagement Email  \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Connected through portfolio Direct communication channel                                                        \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ“– Source 1 - Full Context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> SEO/Organic: If he's ranking for \"Kethan VR,\" he's already thinking about personal branding and discoverability <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> What's missing (but could be added): - Public GitHub projects with 100+ stars (but this could come with scaled  <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> projects) - Shipped product with paying users (but he's 19 and in collegeâ€”time will tell) - Case studies        <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> explaining the problem-solution fit for each project                                                            <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Part 3: Online Presence &amp; Personal Branding                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Multi-Platform Strategy                                                                                         <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Platform URL Strategy Portfolio kethanvr.me Central hub; SEO/discovery GitHub github.com/Kethanvr Code quality  <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> and contributions LinkedIn linkedin.com/in/kethan-vr Professional network, recruiter reach YouTube @kethanvr    <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Long-form tutorials, thought leadership Twitter/X x.com/VrKethan Real-time updates, community engagement Email  <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Connected through portfolio Direct communication channel                                                        <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[34mâ•­â”€\u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34m ğŸ“– Source 2 - Full Context \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Kethan VR: A Comprehensive Research Profile of an Emerging Tech Innovator                                       \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Author: AI Research Analysis Date: January 18, 2026 Location: Bangalore, Karnataka, India Status: Early-Stage   \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Developer & Engineering Student | AI-First Builder                                                              \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Executive Summary                                                                                               \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Kethan VR represents a new breed of Gen Z tech innovators in Indiaâ€”the kind rewriting the startup ecosystem     \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m with bold ideas, shipped products, and an authentic online presence. At 19-20, he's already built a compelling  \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m fullstack portfolio, shipped multiple AI-integrated projects, and positioned himself at the intersection of web \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m development, AI/SaaS, and India's thriving startup culture.                                                     \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Unlike the \"resume polishers,\" Kethan is shipping real stuff: CoCreateAI (an AI prompting platform), MediScan   \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m (AI health assistant), and a portfolio site that actually converts. He's active across GitHub, LinkedIn,        \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m YouTube, and Twitterâ€”building a genuine personal brand without the hype.                                        \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ“– Source 2 - Full Context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Kethan VR: A Comprehensive Research Profile of an Emerging Tech Innovator                                       <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Author: AI Research Analysis Date: January 18, 2026 Location: Bangalore, Karnataka, India Status: Early-Stage   <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Developer &amp; Engineering Student | AI-First Builder                                                              <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Executive Summary                                                                                               <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Kethan VR represents a new breed of Gen Z tech innovators in Indiaâ€”the kind rewriting the startup ecosystem     <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> with bold ideas, shipped products, and an authentic online presence. At 19-20, he's already built a compelling  <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> fullstack portfolio, shipped multiple AI-integrated projects, and positioned himself at the intersection of web <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> development, AI/SaaS, and India's thriving startup culture.                                                     <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Unlike the \"resume polishers,\" Kethan is shipping real stuff: CoCreateAI (an AI prompting platform), MediScan   <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> (AI health assistant), and a portfolio site that actually converts. He's active across GitHub, LinkedIn,        <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> YouTube, and Twitterâ€”building a genuine personal brand without the hype.                                        <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[34mâ•­â”€\u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34m ğŸ“– Source 3 - Full Context \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Startup India: Government grants and mentorship programs                                                        \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Product Hunt: Launch platform for shipping products to 1M+ audience                                             \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Devfolio: Hackathon platform (Kethan likely already active here)                                                \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m Profile Prepared: January 18, 2026 Next Review: July 18, 2026 (6-month check-in)                                \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m This profile is intended as a research document analyzing Kethan VR's position in India's Gen Z startup         \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m ecosystem, his technical positioning, and the macro opportunity he's entering. It's based on public portfolio,  \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ”‚\u001b[0m LinkedIn, GitHub, and broader market trends in Indian startups and AI-first SaaS.                               \u001b[34mâ”‚\u001b[0m\n",
              "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ“– Source 3 - Full Context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Startup India: Government grants and mentorship programs                                                        <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Product Hunt: Launch platform for shipping products to 1M+ audience                                             <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Devfolio: Hackathon platform (Kethan likely already active here)                                                <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Profile Prepared: January 18, 2026 Next Review: July 18, 2026 (6-month check-in)                                <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> This profile is intended as a research document analyzing Kethan VR's position in India's Gen Z startup         <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> ecosystem, his technical positioning, and the macro opportunity he's entering. It's based on public portfolio,  <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> LinkedIn, GitHub, and broader market trends in Indian startups and AI-first SaaS.                               <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
              "<span style=\"color: #000080; text-decoration-color: #000080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "================================================================================\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "================================================================================\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mâ“ Your question:\u001b[0m "
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â“ Your question:</span> </pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1973166094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Get user question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[bold green]â“ Your question:[/bold green] \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Exit condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rich/console.py\u001b[0m in \u001b[0;36minput\u001b[0;34m(self, prompt, markup, emoji, password, stream)\u001b[0m\n\u001b[1;32m   2149\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2151\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q duckduckgo-search beautifulsoup4 requests Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCKV12jgmI3F",
        "outputId": "21376051-fad4-413f-f1f7-3be7c6b5b4d1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.3/3.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/3.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from duckduckgo_search import DDGS\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json\n",
        "\n",
        "class AdvancedRAGWithMemoryVisionWeb:\n",
        "    def __init__(self, mongodb_uri: str, db_name: str = \"rag\", collection_name: str = \"documents\"):\n",
        "        \"\"\"Initialize Advanced RAG with Memory, Vision, and Web capabilities\"\"\"\n",
        "\n",
        "        # 1. Load BGE embedding model\n",
        "        print(\"ğŸ”„ Loading BGE-large model (1024D)...\")\n",
        "        self.embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "        self.embedding_dim = 1024\n",
        "\n",
        "        # 2. Initialize MongoDB connection\n",
        "        print(\"ğŸ”„ Connecting to MongoDB Atlas...\")\n",
        "        self.client = MongoClient(mongodb_uri)\n",
        "        self.db = self.client[db_name]\n",
        "        self.collection = self.db[collection_name]\n",
        "        self.memory_collection = self.db[\"conversation_memory\"]\n",
        "        self.important_info_collection = self.db[\"important_info\"]\n",
        "\n",
        "        # 3. Initialize Gemini 2.5 Flash-Lite with vision support\n",
        "        print(\"ğŸ”„ Initializing Gemini 2.5 Flash-Lite...\")\n",
        "        genai.configure(api_key=GEMINI_API_KEY)\n",
        "        self.llm = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "\n",
        "        # 4. Initialize text splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # 5. Initialize conversation memory (in-session)\n",
        "        self.conversation_history = []\n",
        "        self.session_id = None\n",
        "\n",
        "        print(\"âœ… Advanced RAG System with Memory, Vision & Web initialized!\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # MEMORY FUNCTIONS\n",
        "    # ========================================================================\n",
        "\n",
        "    def start_new_session(self) -> str:\n",
        "        \"\"\"Start a new conversation session\"\"\"\n",
        "        import uuid\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "        self.conversation_history = []\n",
        "        print(f\"ğŸ†• New session started: {self.session_id[:8]}...\")\n",
        "        return self.session_id\n",
        "\n",
        "    def add_to_memory(self, role: str, content: str, metadata: Dict = None):\n",
        "        \"\"\"Add message to conversation memory\"\"\"\n",
        "        message = {\n",
        "            \"role\": role,\n",
        "            \"content\": content,\n",
        "            \"timestamp\": hashlib.sha256(str(np.random.random()).encode()).hexdigest()[:16]\n",
        "        }\n",
        "\n",
        "        if metadata:\n",
        "            message[\"metadata\"] = metadata\n",
        "\n",
        "        self.conversation_history.append(message)\n",
        "\n",
        "        # Store in MongoDB for persistence\n",
        "        memory_doc = {\n",
        "            \"session_id\": self.session_id,\n",
        "            \"role\": role,\n",
        "            \"content\": content,\n",
        "            \"metadata\": metadata,\n",
        "            \"timestamp\": message[\"timestamp\"]\n",
        "        }\n",
        "        self.memory_collection.insert_one(memory_doc)\n",
        "\n",
        "    def get_conversation_context(self, last_n: int = 5) -> str:\n",
        "        \"\"\"Get recent conversation history as context\"\"\"\n",
        "        recent = self.conversation_history[-last_n:]\n",
        "        context = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in recent])\n",
        "        return context\n",
        "\n",
        "    def save_important_info(self, info: str, category: str = \"general\"):\n",
        "        \"\"\"Save important information for long-term memory\"\"\"\n",
        "        doc = {\n",
        "            \"info\": info,\n",
        "            \"category\": category,\n",
        "            \"embedding\": self.embedding_model.encode(info).tolist(),\n",
        "            \"timestamp\": hashlib.sha256(str(np.random.random()).encode()).hexdigest()[:16]\n",
        "        }\n",
        "        self.important_info_collection.insert_one(doc)\n",
        "        print(f\"ğŸ’¾ Saved important info: {info[:50]}...\")\n",
        "\n",
        "    def recall_important_info(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieve important information from long-term memory\"\"\"\n",
        "        query_embedding = self.embedding_model.encode(query).tolist()\n",
        "\n",
        "        pipeline = [\n",
        "            {\n",
        "                \"$vectorSearch\": {\n",
        "                    \"index\": \"memory_index\",\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"queryVector\": query_embedding,\n",
        "                    \"numCandidates\": top_k * 10,\n",
        "                    \"limit\": top_k\n",
        "                }\n",
        "            },\n",
        "            {\"$project\": {\"_id\": 0, \"info\": 1}}\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            results = list(self.important_info_collection.aggregate(pipeline))\n",
        "            return [r['info'] for r in results]\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    # ========================================================================\n",
        "    # VISION FUNCTIONS\n",
        "    # ========================================================================\n",
        "\n",
        "    def analyze_image_from_path(self, image_path: str, question: str = \"Describe this image in detail\") -> str:\n",
        "        \"\"\"Analyze image from file path\"\"\"\n",
        "        try:\n",
        "            with open(image_path, 'rb') as f:\n",
        "                image_data = f.read()\n",
        "\n",
        "            response = self.llm.generate_content([\n",
        "                question,\n",
        "                {\"mime_type\": \"image/jpeg\", \"data\": image_data}\n",
        "            ])\n",
        "\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing image: {e}\"\n",
        "\n",
        "    def analyze_image_from_url(self, image_url: str, question: str = \"Describe this image\") -> str:\n",
        "        \"\"\"Analyze image from URL\"\"\"\n",
        "        try:\n",
        "            response = requests.get(image_url)\n",
        "            image_data = response.content\n",
        "\n",
        "            response = self.llm.generate_content([\n",
        "                question,\n",
        "                {\"mime_type\": \"image/jpeg\", \"data\": image_data}\n",
        "            ])\n",
        "\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing image: {e}\"\n",
        "\n",
        "    def analyze_uploaded_image(self, uploaded_file, question: str = \"Describe this image\") -> str:\n",
        "        \"\"\"Analyze image uploaded in Colab\"\"\"\n",
        "        try:\n",
        "            # For Colab file upload\n",
        "            image_data = uploaded_file[list(uploaded_file.keys())[0]]\n",
        "\n",
        "            response = self.llm.generate_content([\n",
        "                question,\n",
        "                {\"mime_type\": \"image/jpeg\", \"data\": image_data}\n",
        "            ])\n",
        "\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing image: {e}\"\n",
        "\n",
        "    # ========================================================================\n",
        "    # WEB SCRAPING FUNCTIONS\n",
        "    # ========================================================================\n",
        "\n",
        "    def web_search(self, query: str, max_results: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search the web using DuckDuckGo\"\"\"\n",
        "        try:\n",
        "            print(f\"ğŸ” Searching web for: {query}\")\n",
        "            ddgs = DDGS()\n",
        "            results = list(ddgs.text(query, max_results=max_results))\n",
        "            print(f\"âœ… Found {len(results)} results\")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Web search error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def scrape_webpage(self, url: str) -> str:\n",
        "        \"\"\"Scrape and extract text from a webpage\"\"\"\n",
        "        try:\n",
        "            print(f\"ğŸŒ Scraping: {url}\")\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Remove script and style elements\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "\n",
        "            # Get text\n",
        "            text = soup.get_text()\n",
        "\n",
        "            # Clean up text\n",
        "            lines = (line.strip() for line in text.splitlines())\n",
        "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "            print(f\"âœ… Scraped {len(text)} characters\")\n",
        "            return text[:5000]  # Limit to 5000 chars\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Scraping error: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def web_rag(self, query: str, num_results: int = 3) -> str:\n",
        "        \"\"\"Search web, scrape pages, and answer question\"\"\"\n",
        "        # Search web\n",
        "        search_results = self.web_search(query, max_results=num_results)\n",
        "\n",
        "        if not search_results:\n",
        "            return \"No web results found.\"\n",
        "\n",
        "        # Scrape top results\n",
        "        scraped_content = []\n",
        "        for result in search_results[:num_results]:\n",
        "            url = result.get('href', '')\n",
        "            title = result.get('title', '')\n",
        "\n",
        "            if url:\n",
        "                content = self.scrape_webpage(url)\n",
        "                if content:\n",
        "                    scraped_content.append({\n",
        "                        'url': url,\n",
        "                        'title': title,\n",
        "                        'content': content\n",
        "                    })\n",
        "\n",
        "        # Generate answer from scraped content\n",
        "        if not scraped_content:\n",
        "            return \"Could not scrape any content from search results.\"\n",
        "\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Source: {item['title']}\\nURL: {item['url']}\\nContent: {item['content'][:1000]}\"\n",
        "            for item in scraped_content\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"Based on the following web search results, answer the question.\n",
        "\n",
        "Web Content:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer (include source URLs):\"\"\"\n",
        "\n",
        "        response = self.llm.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    # ========================================================================\n",
        "    # ENHANCED GENERATE ANSWER WITH MEMORY\n",
        "    # ========================================================================\n",
        "\n",
        "    def generate_answer_with_memory(self, query: str, use_web: bool = False, top_k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Generate answer using RAG + Memory + optionally Web\"\"\"\n",
        "\n",
        "        # Add query to conversation memory\n",
        "        self.add_to_memory(\"user\", query)\n",
        "\n",
        "        # Get conversation context\n",
        "        conversation_context = self.get_conversation_context(last_n=5)\n",
        "\n",
        "        # Recall important info\n",
        "        recalled_info = self.recall_important_info(query, top_k=2)\n",
        "        recalled_text = \"\\n\".join(recalled_info) if recalled_info else \"\"\n",
        "\n",
        "        # Search documents\n",
        "        print(f\"ğŸ” Searching documents...\")\n",
        "        search_results = self.search(query, top_k=top_k)\n",
        "\n",
        "        # Optionally search web\n",
        "        web_context = \"\"\n",
        "        if use_web:\n",
        "            print(f\"ğŸŒ Searching web...\")\n",
        "            web_context = self.web_rag(query, num_results=2)\n",
        "\n",
        "        # Prepare context\n",
        "        doc_context = \"\"\n",
        "        if search_results:\n",
        "            context_chunks = [result['text'] for result in search_results]\n",
        "            doc_context = \"\\n\\n\".join([f\"[Doc {i+1}]\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
        "\n",
        "        # Create enhanced prompt with memory\n",
        "        prompt = f\"\"\"You are a helpful AI assistant with memory of our conversation.\n",
        "\n",
        "Conversation History:\n",
        "{conversation_context}\n",
        "\n",
        "Recalled Important Information:\n",
        "{recalled_text}\n",
        "\n",
        "Document Context:\n",
        "{doc_context}\n",
        "\n",
        "Web Context:\n",
        "{web_context}\n",
        "\n",
        "Current Question: {query}\n",
        "\n",
        "Instructions:\n",
        "- Answer based on all available context\n",
        "- Reference previous conversation if relevant\n",
        "- If using web info, cite sources\n",
        "- Be conversational and remember our chat history\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Generate answer\n",
        "        print(\"ğŸ¤– Generating answer...\")\n",
        "        try:\n",
        "            response = self.llm.generate_content(prompt)\n",
        "            answer = response.text\n",
        "        except Exception as e:\n",
        "            answer = f\"Error generating answer: {e}\"\n",
        "\n",
        "        # Add answer to memory\n",
        "        self.add_to_memory(\"assistant\", answer)\n",
        "\n",
        "        # Check if answer contains important info to save\n",
        "        if \"important\" in query.lower() or \"remember\" in query.lower():\n",
        "            self.save_important_info(answer, category=\"user_marked\")\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": search_results,\n",
        "            \"web_used\": use_web,\n",
        "            \"conversation_context\": conversation_context,\n",
        "            \"recalled_info\": recalled_info,\n",
        "            \"num_sources\": len(search_results)\n",
        "        }\n",
        "\n",
        "    # ========================================================================\n",
        "    # EXISTING FUNCTIONS (keep your ingest_file, search, etc.)\n",
        "    # ========================================================================\n",
        "\n",
        "    def load_file(self, file_path: str) -> List[Document]:\n",
        "        \"\"\"Load any file format using LangChain loaders\"\"\"\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "        print(f\"ğŸ“„ Loading {file_extension} file...\")\n",
        "\n",
        "        try:\n",
        "            if file_extension == '.pdf':\n",
        "                loader = PyPDFLoader(file_path)\n",
        "            elif file_extension in ['.docx', '.doc']:\n",
        "                loader = Docx2txtLoader(file_path)\n",
        "            elif file_extension == '.txt':\n",
        "                loader = TextLoader(file_path)\n",
        "            elif file_extension in ['.xlsx', '.xls']:\n",
        "                loader = UnstructuredExcelLoader(file_path)\n",
        "            elif file_extension == '.csv':\n",
        "                loader = CSVLoader(file_path)\n",
        "            elif file_extension == '.md':\n",
        "                loader = UnstructuredMarkdownLoader(file_path)\n",
        "            elif file_extension in ['.html', '.htm']:\n",
        "                loader = UnstructuredHTMLLoader(file_path)\n",
        "            else:\n",
        "                print(f\"âš ï¸ Unsupported file format: {file_extension}\")\n",
        "                return []\n",
        "\n",
        "            documents = loader.load()\n",
        "            print(f\"âœ… Loaded {len(documents)} page(s)/section(s)\")\n",
        "            return documents\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading file: {e}\")\n",
        "            return []\n",
        "\n",
        "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split documents into chunks\"\"\"\n",
        "        print(\"ğŸ”„ Splitting documents...\")\n",
        "        chunks = self.text_splitter.split_documents(documents)\n",
        "        print(f\"âœ… Created {len(chunks)} chunks\")\n",
        "        return chunks\n",
        "\n",
        "    def generate_hash(self, text: str) -> str:\n",
        "        \"\"\"Generate SHA-256 hash\"\"\"\n",
        "        return hashlib.sha256(text.encode()).hexdigest()\n",
        "\n",
        "    def ingest_file(self, file_path: str, metadata: Dict[str, Any] = None):\n",
        "        \"\"\"Ingest file with deduplication\"\"\"\n",
        "        if metadata is None:\n",
        "            metadata = {}\n",
        "\n",
        "        documents = self.load_file(file_path)\n",
        "        if not documents:\n",
        "            return\n",
        "\n",
        "        chunks = self.chunk_documents(documents)\n",
        "\n",
        "        print(\"ğŸ”„ Generating embeddings and storing...\")\n",
        "        stored = 0\n",
        "        skipped = 0\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            embedding = self.embedding_model.encode(chunk.page_content).tolist()\n",
        "            chunk_hash = self.generate_hash(chunk.page_content)\n",
        "\n",
        "            if self.collection.find_one({\"hash\": chunk_hash}):\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            doc = {\n",
        "                \"text\": chunk.page_content,\n",
        "                \"embedding\": embedding,\n",
        "                \"hash\": chunk_hash,\n",
        "                \"metadata\": {**metadata, **chunk.metadata, \"chunk_index\": i}\n",
        "            }\n",
        "\n",
        "            self.collection.insert_one(doc)\n",
        "            stored += 1\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"  âœ“ {i + 1}/{len(chunks)} chunks...\")\n",
        "\n",
        "        print(f\"âœ… Stored: {stored}, Skipped: {skipped}\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Vector search\"\"\"\n",
        "        query_embedding = self.embedding_model.encode(query).tolist()\n",
        "\n",
        "        pipeline = [\n",
        "            {\n",
        "                \"$vectorSearch\": {\n",
        "                    \"index\": \"vector_index\",\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"queryVector\": query_embedding,\n",
        "                    \"numCandidates\": top_k * 10,\n",
        "                    \"limit\": top_k\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"$project\": {\n",
        "                    \"_id\": 0,\n",
        "                    \"text\": 1,\n",
        "                    \"metadata\": 1,\n",
        "                    \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            results = list(self.collection.aggregate(pipeline))\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Search error: {e}\")\n",
        "            return []\n",
        "\n",
        "print(\"âœ… AdvancedRAGWithMemoryVisionWeb class ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "himZqPYXmfuW",
        "outputId": "ca291dfe-4792-4a7b-c17f-c3bdc04d43ca"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… AdvancedRAGWithMemoryVisionWeb class ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸš€ INITIALIZING ENHANCED RAG SYSTEM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rag = AdvancedRAGWithMemoryVisionWeb(\n",
        "    mongodb_uri=MONGODB_URI,\n",
        "    db_name=\"rag\",\n",
        "    collection_name=\"rag-collection\"\n",
        ")\n",
        "\n",
        "# Start a new conversation session\n",
        "rag.start_new_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "MIWKg8y6mq-1",
        "outputId": "f1e62c01-eac8-49c9-b06d-e7c18cc727d2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸš€ INITIALIZING ENHANCED RAG SYSTEM\n",
            "================================================================================\n",
            "ğŸ”„ Loading BGE-large model (1024D)...\n",
            "ğŸ”„ Connecting to MongoDB Atlas...\n",
            "ğŸ”„ Initializing Gemini 2.5 Flash-Lite...\n",
            "âœ… Advanced RAG System with Memory, Vision & Web initialized!\n",
            "ğŸ†• New session started: bbbf3dfc...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bbbf3dfc-ac74-417d-8936-d9ffd3e0233b'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ’¬ ENHANCED RAG SYSTEM - Memory + Vision + Web\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "Commands:\n",
        "- Just ask a question (uses documents + memory)\n",
        "- 'web: your question' (search web + scrape)\n",
        "- 'image' (analyze an image)\n",
        "- 'remember: info' (save to long-term memory)\n",
        "- 'recall: topic' (retrieve from memory)\n",
        "- 'exit' (quit)\n",
        "\"\"\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nâ“ You: \").strip()\n",
        "\n",
        "    if user_input.lower() in ['exit', 'quit', 'q']:\n",
        "        print(\"ğŸ‘‹ Goodbye!\")\n",
        "        break\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "    # Handle different commands\n",
        "    if user_input.startswith('web:'):\n",
        "        # Web search mode\n",
        "        query = user_input[4:].strip()\n",
        "        result = rag.generate_answer_with_memory(query, use_web=True)\n",
        "        print(f\"ğŸ¤– Answer:\\n{result['answer']}\\n\")\n",
        "        print(f\"ğŸŒ Web search: Enabled\")\n",
        "        print(f\"ğŸ“š Documents used: {result['num_sources']}\")\n",
        "\n",
        "    elif user_input.lower() == 'image':\n",
        "        # Image analysis mode\n",
        "        print(\"ğŸ“¤ Upload an image...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if uploaded:\n",
        "            question = input(\"â“ What do you want to know about this image? \")\n",
        "            answer = rag.analyze_uploaded_image(uploaded, question)\n",
        "            print(f\"\\nğŸ¤– Image Analysis:\\n{answer}\\n\")\n",
        "\n",
        "            # Add to memory\n",
        "            rag.add_to_memory(\"user\", f\"Analyzed image: {question}\")\n",
        "            rag.add_to_memory(\"assistant\", answer)\n",
        "\n",
        "    elif user_input.startswith('remember:'):\n",
        "        # Save to long-term memory\n",
        "        info = user_input[9:].strip()\n",
        "        rag.save_important_info(info)\n",
        "        print(\"ğŸ’¾ Saved to long-term memory!\")\n",
        "\n",
        "    elif user_input.startswith('recall:'):\n",
        "        # Recall from memory\n",
        "        topic = user_input[7:].strip()\n",
        "        recalled = rag.recall_important_info(topic)\n",
        "        if recalled:\n",
        "            print(\"ğŸ§  Recalled from memory:\")\n",
        "            for i, info in enumerate(recalled, 1):\n",
        "                print(f\"\\n{i}. {info}\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Nothing found in memory\")\n",
        "\n",
        "    else:\n",
        "        # Normal RAG with memory\n",
        "        result = rag.generate_answer_with_memory(user_input, use_web=False)\n",
        "        print(f\"ğŸ¤– Answer:\\n{result['answer']}\\n\")\n",
        "\n",
        "        if result['num_sources'] > 0:\n",
        "            print(f\"ğŸ“š Sources: {result['num_sources']}\")\n",
        "\n",
        "            show = input(\"View sources? (y/n): \").strip().lower()\n",
        "            if show == 'y':\n",
        "                for i, source in enumerate(result['sources'], 1):\n",
        "                    print(f\"\\n[Source {i}]\")\n",
        "                    print(f\"File: {source['metadata'].get('source', 'Unknown')}\")\n",
        "                    print(f\"Score: {source.get('score', 0):.4f}\")\n",
        "\n",
        "    print(\"-\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "a-IpVPmqmvy5",
        "outputId": "851d3fed-40d9-4d56-e47f-74e8b634eb91"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ’¬ ENHANCED RAG SYSTEM - Memory + Vision + Web\n",
            "================================================================================\n",
            "\n",
            "Commands:\n",
            "- Just ask a question (uses documents + memory)\n",
            "- 'web: your question' (search web + scrape)\n",
            "- 'image' (analyze an image)\n",
            "- 'remember: info' (save to long-term memory)\n",
            "- 'recall: topic' (retrieve from memory)\n",
            "- 'exit' (quit)\n",
            "\n",
            "\n",
            "â“ You: hi\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ğŸ” Searching documents...\n",
            "ğŸ¤– Generating answer...\n",
            "ğŸ¤– Answer:\n",
            "Hi there! How can I help you today?\n",
            "\n",
            "ğŸ“š Sources: 3\n",
            "View sources? (y/n): y\n",
            "\n",
            "[Source 1]\n",
            "File: /tmp/Kethan VR A Comprehensive.md\n",
            "Score: 0.7573\n",
            "\n",
            "[Source 2]\n",
            "File: /tmp/Kethan VR A Comprehensive.md\n",
            "Score: 0.7533\n",
            "\n",
            "[Source 3]\n",
            "File: /tmp/Kethan VR A Comprehensive.md\n",
            "Score: 0.7503\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    }
  ]
}